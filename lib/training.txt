This week, we launched the Power Up Your Donation campaign. For the next few days, donations to EFF through the campaign will be matched by challenge grants from passionate digital rights advocates. Donate today and double your impact on securing networks and devices, stopping illegal government surveillance, fighting censorship, protecting the freedom to tinker, and more!  When someone participates in a campaign like Power Up Your Donation, they are building on a pillar of EFF's approach to working for a better digital future: the integrity and responsibility of being a member-supported non-profit. We're dedicated to bringing the very best legal, technical, and creative expertise to tackle threats to our essential rights and freedoms. And for years, that expertise has been honed and made exceptional by the passion shared by EFF members. With financial support, moral support, and their very voices, EFF members speak the truth—and sometimes, even just common sense—to the large and powerful institutions looking to shape our future. In the Power Up Your Donation campaign, we're asking for your help, and our matching fund donors are standing with us to commend those who join EFF at this critical time. Together, we can take this year's momentum and do even more, with a full docket of cases and projects to promote encryption, fight censorship, protect privacy, and end DRM. Join the Power Up Your Donation campaign and double your impact with a one-to-one match! In 1998, the U.S. Patent and Trademark Office issued Patent 5,718,632, on a method for avoiding “unnecessary wastage of time” in video games. What’s transpired in the 17 years since then can best be described as an unnecessary wastage of time. Namco’s patent covers “auxiliary games” that a player can enjoy while the main game is loading. The patent expired on November 27, which has generated a lot of excitement in the gaming world, and even inspired a Loading Screen Jam where developers create their own loading screen games. In the midst of all the excitement, it’s worth taking a moment to ask whether Patent 5,718,632 should have been issued in the first place. It’s emblematic of a big problem with a lot of patents, particularly software patents.  The first Sony PlayStation was introduced in 1994. Its graphical capabilities blew predecessors like the Sega Genesis and Super Nintendo out of the water, but it had one big disadvantage. It replaced the game cartridges of the previous generation with CD-ROMs. When you booted up a PlayStation game, you had to wait for the console to load game data from the disc into its own memory. And that. Took. For. Ever. Watching a loading screen was boring, especially when you were used to the instant gratification of cartridge games. Namco’s Ridge Racer addressed the problem by including a second game, the 80s classic Galaxian. It took no time at all for a PlayStation to load Galaxian. Suddenly, the player wasn’t thinking about how boring it was to wait for a game to load; she could have fun playing Galaxian while the console took its time loading Ridge Racer. If she beat Galaxian before Ridge Racer was done loading, she’d be rewarded in Ridge Racer with access to some in-game bonuses. What’s the big deal? Namco thought of loading screen games first, so they earned the patent, right? Well, let’s look at how U.S. law defines a patentable invention. According to the law, a person isn’t entitled to a patent if the claimed invention already existed when the application was filed or would have been obvious to someone skilled in the relevant technology area. The idea of playing a small game while the larger one loads has been around for a very long time. In 1987, many years before Namco filed its patent application, Richard Aplin created Invade-a-Load, a utility for developers who wrote games for the Commodore 64 computer. As a game developer, you could package Invade-a-Load with your game; while players waited for the game, they’d be treated to a mini-game similar to Space Invaders. Given the breadth of Namco’s claims, there is a very strong case that its application should have been rejected as anticipated or obvious in light of Invade-a-Load.  The purpose of patents, as spelled out in the U.S. Constitution, is to promote innovation. Inventors are rewarded with a temporary monopoly on their invention in exchange for giving the public information on how it works. The Supreme Court has said that patents should only be granted for “those inventions which would not be disclosed or devised but for the inducement of a patent.” In other words, if the inventor isn’t trading information of real value for the patent, then it’s not a good patent. Unfortunately, many software patents do just the opposite. They offer no real information about how to implement a feature, only a vaguely worded description of the feature itself. In court, vagueness is often rewarded over specificity, as plaintiffs stretch the boundaries of their inventions to cover the defendants’ products. As for the auxiliary game patent, it simply describes the idea of loading a separate game while the player waits for the main game. Namco gave no information of value in return for its monopoly on auxiliary games. Last year, the Supreme Court struck down a patent for describing nothing but an abstract idea performed on a computer. If Namco had sued someone over its patent, there’s a good chance it would have lost. But it didn’t matter: even the existence of these low quality, abstract software patents is enough to deter a lot of innovators. There are two patent reform bills currently in Congress, the PATENT Act and the Innovation Act. EFF supports both bills, and we hope you’ll take a moment to encourage your members of Congress to support them too. Those bills mostly only address patent litigation, though. If we really want to make the patent system the agent of innovation that the Constitution calls for, we need to tackle the problem of patent quality. Right now, software patents are the loading screen of the intellectual property world. They add nothing of value to the game. They just force everyone to sit and wait before we can do anything. The United States Court of Appeals for the Second Circuit issued an opinion rejecting the government’s attempt to hold an employee criminally liable under the federal hacking statute—the Computer Fraud and Abuse Act (“CFAA”)—for violating his employer-imposed computer use restrictions. The decision is important because it ensures that employers and website owners don’t have the power to criminalize a broad range of innocuous everyday behaviors, like checking personal email or the score of a baseball game, through simply adopting use restrictions in their corporate policies or terms of use. The court also ruled that the government cannot hold people criminally liable on the basis of purely fantastical statements they make online—i.e., thoughtcrime.   The case, United States v. Gilberto Valle, received a lot of attention in the press because it involved the so-called “cannibal cop”—a New York City police officer who was charged with conspiracy to kidnap for posts he wrote on fetish websites about cannibalism. Valle was also charged with violating the CFAA for accessing a police database to look up information about people without a valid law enforcement purpose, in violation of NYPD policy. The jury convicted Valle on all counts, but the trial court reversed the jury’s conspiracy verdict, stating that “the nearly yearlong kidnapping conspiracy alleged by the government is one in which no one was ever kidnapped, no attempted kidnapping ever took place, and no real-world, non-Internet-based steps were ever taken to kidnap anyone.” The trial court ultimately found that holding Valle guilty of conspiracy to kidnap would make him guilty of thoughtcrime.  But the trial court upheld the CFAA conviction. And on appeal, we filed an amicus brief with the Second Circuit, urging the court to overturn the lower court’s dangerous ruling.  We argued that the lower court’s ruling would make criminals out of millions of innocent individuals, and the Second Circuit agreed—throwing out Mr. Valle's CFAA conviction and joining two other federal circuit courts in rejecting the government’s attempt to expand the reach of the vaguely worded federal statute: “We decline to adopt the prosecution’s construction [of the CFAA], which would criminalize the conduct of millions of ordinary computer users[.]”  The court went on: While the Government might promise that it would not prosecute an individual for checking Facebook at work, we are not at liberty to take prosecutors at their word in such matters. A court should not uphold a highly problematic interpretation of a statute merely because the Government promises to use it responsibly. The Second Circuit also upheld the trial court’s decision to throw out the conspiracy conviction, as we had urged in a second amicus brief filed in the case, holding that “[t]he mere indulgence of fantasy, even of the repugnant and unsettling kind here, is not, without more, criminal.” Thanks again to the Center for Democracy & Technology, the National Association of Criminal Defense Lawyers, and the Internet scholars who joined our CFAA amicus brief, and to UCLA law professor Eugene Volokh of the Scott & Cyan Banister First Amendment Clinic for writing our amicus brief regarding the conspiracy charges. At EFF we put security and privacy first. This means working hard at keeping our members and site visitors safe, as well as the people who use the software we develop. We also dedicate staff time to advising security researchers, maintaining resources like our Coders' Rights Project, and helping groups like Facebook improve their bug reporting policies. Today we're following our own advice by announcing EFF's own Security Vulnerability Disclosure Program. The Disclosure Program is a set of guidelines on how to report bugs in software EFF develops, like HTTPS Everywhere or Let's Encrypt, as well as the software we use to run our sites and services. The scope of the bugs we're looking for is detailed on the Security Vulnerability Disclosure Program page, but we're not just looking for bugs in our code. Security vulnerabilities created by the specific configuration of software on EFF servers are also within the scope of this program. One difference between our program and others is that as a lean, member-driven nonprofit we don't have the resources to match the cash rewards others can provide for zero days. Instead, what we can offer is public acknowledgement on our EFF Security Hall of Fame page and other non-cash rewards like EFF gear or complimentary EFF memberships. But reporting bugs does more than just help EFF and earn you cool swag. Coordinated disclosure helps us keep the NSA from exploiting zero days like Heartbleed, and as an organization committed to using and developing free software whenever possible, letting us know about bugs will help us work with upstream software developers to get a fix for impacted users. Security research is a prerequisite for safe computing. We're lucky to have such a talented base of supporters and members who can donate their time to help us improve online security, so we invite you to help us by inspecting, analyzing, and improving the code we write. We especially want to encourage security researchers to turn their attention towards the beta release of the Let's Encrypt Client (the master branch of the linked repo). As an added incentive, we're currently brainstorming even neater rewards which we may only give out for vulnerabilities in that software. In order to get started, visit our Security Vulnerability Disclosure Program page to view the full reporting guidelines. And don't forget to download a copy of the GPG key to use when submitting your vulnerabilities. Happy hunting! The FBI recently opened beta testing of eFOIA, a new online system for filing and tracking requests for records under the Freedom of Information Act (FOIA). On first glance, the project seems like a noble effort to streamline transparency in an agency that is notoriously slow and resistant to releasing public information. But there’s one feature that we would like to see treated as a bug and excised from the new system: mandatory submission of government-issued identification. Each time you want to file a FOIA request through eFOIA, you are required to upload a scan of your driver’s license or other formal identification card. The FBI offers very little reasoning, except to say that it is “So the FBI is confident in the identity of the requester.” The FBI does not explain how people’s ID cards will be stored, used, or shared, except under its rather broad existing site-wide privacy policy. To be clear: nothing in the FOIA statute requires requesters to show a government-issued ID for a standard FOIA request, and we’re not sure it’s even legal to ask for it. More than 20 years ago, a federal appeals court ruled that, even when a FOIA request requires a privacy waiver, the person needs only to attest under penalty of perjury that they are who they say they are. The Department of Homeland Security’s own eFOIA app, which was designed with the same intent as the FBI system, follows the court’s guidance. It doesn’t require ID, but if you are requesting “Personal Records” (for example, under the federal Privacy Act), you need to check a box declaring on penalty of perjury that all the information you’ve supplied is true, and then sign it electronically. Many states, such as New Jersey and California, are very clear in their policies that you can file requests for information anonymously. Just last year, a state appeals court in Florida agreed that requiring identifying information from an anonymous requester would have a “chilling effect on access to public records,” under the state’s Public Records Act. Requiring photo ID will undoubtedly have a chilling effect, at least when it comes to using the FBI’s new system. The public remains rightfully suspicious of an agency that has amassed information on individuals simply exercising their First Amendment rights as far back as the J. Edgar Hoover administration. Researchers, journalists, and activists may avoid the system for fear that it will result in further government scrutiny over their private lives. After all, many FOIA requests are intended to uncover controversial actions that could embarrass the FBI. Many people who file FOIA requests use their real name, but supply the address of where they work or a P.O. Box.  By demanding photo ID, the FBI is asking these people to hand over their home addresses as well, along with a photograph, and their sex, weight, height, and hair and eye color. For the purpose of processing a FOIA request, there’s no reason at all that the FBI needs to know whether you need corrective lenses to drive.  Of even further concern is whether the photo ID you submit may be submitted to or compared against the FBI’s massive facial recognition database that we reported on in September. It’s so far unclear how this photo requirement will affect the FOIA assistance services (e.g. MuckRock, FOIA Machine, and private law firms) who file or manage requests on the behalf of others. Many could be interested in using the eFOIA interface, but that would require these organizations to solicit driver license scans from their clients, adding a whole new point in the process where sensitive information could be compromised. The FBI should not be collecting any more information than it absolutely needs to process a request. With FOIA, it’s the public who gets to ask “papers, please,” not the other way around.  Multiple recent reports on serious security vulnerabilities in cable modems and routers paint a dire picture of the state of security of the devices that millions of users depend upon to connect to the Internet. Such vulnerabilities can be exploited to disable our access, snoop on our personal information, or launch malicious attacks on third parties. Other devices that are equally important for our security, or even to our physical health and safety—such as home alarm systems and, terrifyingly, a cardio server used in hospitals—have also been the subject of recent vulnerability disclosures. One tool that security researchers can use to more quickly uncover and eliminate such vulnerabilities is having access to the source code of the software embedded in these devices. Of course, that can usually only be done if the source code is made available to them by the supplier. Many router manufacturers do make at least some of their devices' source code available, and often they do so because they are legally compelled to do this by the terms of the GNU General Public License, which applies to some of the core software upon which such devices are frequently based. But that's not the only way that the manufacturers of critical devices could be compelled to release their code for public or peer review. There's also the option that a law or regulation could be made requiring the disclosure of such code, perhaps as a condition of the licensing of the products under applicable law. In fact, in October, 260 cybersecurity experts called upon the Federal Communications Commission to impose just such a requirement. Which brings us to the Trans-Pacific Partnership (TPP) agreement—which would prohibit such open source or code audit mandates being introduced in the future. Article 14.17 of the text of the Electronic Commerce chapter provides, “No Party shall require the transfer of, or access to, source code of software owned by a person of another Party, as a condition for the import, distribution, sale or use of such software, or of products containing such software, in its territory.” As indicated above, this isn't just an issue confined to routers and modems. It could also apply to medical devices, smoke alarms, drink mixers, motor vehicles such as cars and tractors, wearables, and not to mention a myriad of pure software applications running on smartphones and PCs. Only devices and software used in bespoke applications (not for a mass market) or in critical infrastructure would be exempt under the terms of the TPP language, though the precise ambit of these exemptions remains unclear. The provision would do more than merely outlaw open source code mandates. It would also prohibit any requirement that code be submitted for private review by regulatory authorities. Although we love free and open source software, it's not for everyone. So an alternative to requiring the vendors of proprietary software to release their code to the general public would be for them to have it audited by the responsible licensing authority, health and safety watchdog, or consumer protection agency. This possibility is also foreclosed by the TPP's strict language. And the bad keeps on coming. Not only does the TPP foreclose rules that could require code to be open sourced or audited, it could also make it impossible for competition authorities to open up the market for the repair of products with embedded software. If the manufacturer of a car can't be required to give others access to the source code of the software that runs on its embedded computer systems, this seriously hinders an entire market for independent repair mechanics who could compete with its authorized repairers to work on that code, as well as markets for entrepreneurs to use their understanding of that code to make new devices that interoperate with vehicles, such as diagnostic tools and smartphone applications. This carries significant competition implications that the TPP negotiators, so far as we know, didn't even consider during their closed door negotiations in luxury hotels around the world. Why would the United States wish to tie its own hands in this way? On the face of it, in an environment where the Internet of Things is burgeoning and software quality is headline news, this restriction to our future regulatory capacity seems to make no sense at all. The answer is found in this year's edition of the U.S. Trade Representative's (USTR) Special 301 Report, in which the USTR calls out China for measures requiring the disclosure of source code to the Chinese government (though oddly this complaint is only levied in the accompanying press release, and not in the PDF report itself). Although described by the USTR as “new,” these requirements actually date from 2007 and are part of China's framework regulations for information security in critical infrastructure, known as the Multi-Level Protection Scheme (MLPS). The MLPS regulations limit products from being sold for use in Chinese information systems above a certain security level, unless their source code is disclosed to the government. Although this measure is presented as protection against security flaws and deliberate backdoors being inserted into critical software, it is also seen by U.S. companies as an impingement upon their ability to keep their code proprietary. Assuming that this Chinese regulation is, in fact, a legitimate problem for U.S. companies, does the TPP actually address this very narrow problem? Not at all. First and most obviously, China is not a party to the TPP, and isn't likely to become one any time soon. But even if it were, the MLPS regulations only apply to software used in critical infrastructure—which is expressly exempted from the TPP provision anyway. So if anything, the provision makes even less sense than it seems to make at first glance. No matter what you may think of the wisdom of laws or regulations requiring the disclosure of audit of source code in particular cases, we should all be able to agree that this is a complicated issue, and one that, as the Washington Post points out, shouldn't be resolved through a trade agreement negotiated behind closed doors. Since this controversial provision is actually touted by the USTR as a benefit to the tech industry from the deal, it goes to show once again what a mess they have made of this agreement by keeping actual users and innovators locked out of the negotiating room. ~ If you're in the United States, urge your lawmakers to call a hearing on the contents of the TPP that will impact your digital rights, and more importantly, to vote this deal down when it comes to them for ratification:  Today marks a major milestone for the encrypted Web. Let's Encrypt, the free and automated certificate authority, has entered Public Beta. That means it's easier than ever for websites to adopt HTTPS encryption. A huge percentage of the world's daily Internet usage currently takes place over unecrypted HTTP, exposing people to illegal surveillance and injection of unwanted ads, malware, and tracking headers into the websites they visit. EFF's Encrypt the Web project aims to fix that, and Let's Encrypt—a collaboration with Mozilla, the University of Michigan, Cisco, Akamai and many other sponsoring organizations—should be a huge step forward. In order to use HTTPS, a website operator needs to obtain and install a certificate: a file that is digitally signed by a certificate authority (CA). The certificate contains the cryptographic keys necessary to securely communicate with that particularly website. There are a number of flaws in the CA system, but when it comes to encrypting the Web, two in particular stand out: cost and difficulty. Most CAs today charge for certificates. While some are very cheap, every dollar of expense means a large swath of people who can't afford to host a secure website. The larger barrier, though, is difficulty.  Once someone has purchased a certificate, they need to install it on their website, a time consuming and error-prone process that requires significant technical skill, which is a cost in itself. Let's Encrypt is not only free but also automated, in order to make HTTPS encryption more accessible than ever. So if you run a server, and need certificates to deploy HTTPS, you can run the beta client and get one right now.  If you have any questions, you can get answers on community.letsencrypt.org. We've still got a lot to do. This launch is a Public Beta to indicate that, as much as today's release makes setting up HTTPS easier, we still want to make a lot more improvements towards our ideal of fully automated server setup and renewal. Our roadmap includes may features including options for complete automation of certificate renewal, support for automatic configuration of more kinds of servers (such as Nginx, postfix, exim, or dovecot), and tools to help guide users through the configuration of important Web security features such as HSTS, upgrade-insecure-requests, and OCSP Stapling.  And of course, if you have some Python coding knowledge, you can come and help us reach those objectives. A fully encrypted Web is within reach. Let's Encrypt is going to help us get there. It’s easy to file a patent complaint. All a patent owner has to do is say that they own a patent and that the defendant infringed it. The patent holder doesn’t even need to identify which product of the defendant’s they believe infringes the patent, or specify which claims of the patent they’re asserting. It’s an absurdly simple process, and unscrupulous patent tolls routinely take advantage of that fact. That might have changed this week—the Judicial Conference of the United States has instituted a rule change that includes eliminating the form that’s been used for patent complaints for decades. We hope that the change makes it harder for patent trolls to hit defendants with information-free complaints, but we’re not breaking out the Champagne yet. This story starts in 2007 with the Supreme Court’s decision in Bell Atlantic Corp. v. Twombly. In that case, the Supreme Court raised the standard for pleading in civil trials, suggesting that fuzzy complaints give the plaintiff an unfair advantage. Justice David Souter wrote, “[T]he threat of discovery expense will push cost-conscious defendants to settle even anemic cases” before litigants get to the discovery process (when litigants turn over evidence to each other). Souter laid out a standard that complaints should carry “enough facts to state a claim to relief that is plausible on its face.” The ruling sent reverberations around the legal world: judges began requiring more information when plaintiffs filed a suit, and dismissing cases where they thought those details were lacking (more on that in a minute). But the Federal Circuit—the court that hears patent appeals—decided not to apply it at all to allegations of patent infringement. Here’s where it gets weird. The reason why the Federal Circuit excepted patent cases from Twombly is because of Form 18. It sounds like something out of a Joseph Heller novel, but Form 18 was a template for patent complaints included as part of an appendix to the Federal Rules of Civil Procedure. The form was written in 1938 and hasn’t changed much since. It’s barely over a page long, with a space for the plaintiff to enter the patent or patents that it thinks the defendant has infringed, and another for the type of product the defendant offers that allegedly uses the plaintiff’s invention. That’s the whole form. As Charles Duan put it, “A patent owner can simply name defendants and a few patent numbers on the complaint, and that is enough to get in the courtroom door.” Thanks in part to Form 18, patent trolls have been able to use bare bones complaints to file cases cheaply and then place expensive discovery burdens on defendants. A company facing a thin complaint has to go through discovery just to find out what it is being sued for. And since patent trolls tend to be shell companies with few documents and witnesses, the discovery process can be far more expensive for defendants than for plaintiffs. That creates intense pressure to settle even frivolous suits. In K-Tech Telecommunications v. Time Warner (2013), the Federal Circuit ruled that Form 18’s minimal requirements were still adequate after Twombly. It held that “to the extent that any conflict exists between Twombly (and its progeny) and the Forms regarding pleadings requirements, the Forms control.” Only in the baffling world of patent litigation does a Microsoft Word template get precedence over a Supreme Court ruling. Since K-Tech, a notable imbalance has emerged in the way courts consider different kinds of cases. While the higher pleading standards in Twombly have been mostly ignored in patent litigation, they’re arguably overused in civil liberties cases. Just a few weeks ago, a court threw out a suit that Wikimedia and several other plaintiffs had brought against the NSA. Wikimedia alleged that the NSA had violated the Constitution by intercepting Americans’ international communications. The court cited Twombly, saying that because Wikimedia and co. didn’t have specific evidence that their or their users’ communications had been monitored, they didn’t have the right to bring a lawsuit against the NSA. Forget that through a combination of leaked documents and the NSA’s own admissions, we know that the NSA scans a substantial portion of upstream Internet traffic, and that Wikimedia et al. represented some of the most traffic-heavy parts of the Internet. It is extremely unlikely, if not impossible, that the NSA’s dragnet didn’t capture some Wikimedia traffic. Nonetheless, because Wikimedia couldn’t identify a specific communication that the NSA intercepted, or state with 100% certainty precisely how the NSA conducts its upstream surveillance, the court refused to hear the case. The way in which Twombly is interpreted generates imbalance in the justice system. Some plaintiffs, like those in the Wikimedia case, are effectively asked to prove their claims before they’re even allowed into court. Other plaintiffs, like patent owners, don’t even have to name an allegedly infringing product (let alone provide specific allegations about how the product infringes one of the patent claims). It seems that Twombly’s significance varies a lot depending whose rights are at stake. On December 1, the Judicial Conference of the United States made several changes to the FRCP, including completely removing the appendix of forms. Since the Federal Circuit’s refusal to follow Twombly rested entirely on the existence of Form 18, this change should mean that parties that sue for patent infringement will now be held to the same standard that has developed for other types of litigation. In a recent interview, patent litigator Chris Mammen said, “The cookie-cutter, cut-and-paste complaint is on its way out. Patent plaintiffs will have to allege more facts and it marks an end to plaintiffs suing multiple defendants.” We'll have to wait and see how courts apply the new standards in practice, but here's one interesting sign: there was a huge jump in patent lawsuits filed in November, with many of them filed by non-practicing entities. On Monday, November 30, trolls set a new record for most suits filed in one day. Clearly, a lot of plaintiffs wanted to get in the door ahead of the rule change. The death of Form 18 might also put more power into the hands of district court judges. As we’ve discussed many times, federal district courts are not all the same. The Eastern District of Texas has used local rules and practices to make itself an attractive venue for patent trolls. It’s not a stretch to think that patent trolls will be looking for the courts that interpret the new rules in a troll-friendly fashion. As long as trolls get to decide where to file a suit, any improvement in the law will only be as strong as the worst court’s interpretation of it. No posts found Many media reports on (as well as at least one response to) the FTC complaint we submitted yesterday about Google’s violation of the Student Privacy Pledge have focused heavily on one issue—Google’s use of Chrome Sync data for non-educational purposes. This is an important part of our complaint, but we want to clarify that Google has other practices which we are just as concerned about, if not more so. In particular, the primary thrust of our complaint focuses on how Google tracks and builds behavioral profiles on students when they navigate to Google-operated sites outside of Google Apps for Education. We’ve tried to explain this issue in both our complaint and our FAQ, but given its significance we think it’s worth explaining again. To understand what’s going on, you first have to understand that when it comes to education, Google divides its services into two categories: Google Apps for Education (GAFE), which includes email, Calendar, Talk/Hangouts, Drive, Docs, Sheets, Slides, Sites, Contacts, and the Apps Vault; and everything else, which includes Google Search, Blogger, Bookmarks, Books, Maps, News, Photos, Google+, and YouTube, just to name a few. Google has promised not to build profiles on students or serve them ads only within Google Apps for Education services. When a student goes to a different Google service, however, and they’re still logged in under their educational account, Google associates their activity on that service with their educational account, and then serves them ads on at least some of those non-GAFE services based on that activity. In other words, when a student logs into their educational account, and then uses Google News to create a report on current events, or researches history using Google Books, or has a geography lesson using Google Maps, or watches a science video on YouTube, Google tracks that activity and feeds it into an ad profile attached to the student’s educational account—even though Google knows that the person using that account is a student, and the account was created for educational purposes. This is our biggest complaint about Google’s practices—that despite having promised not to track students, Google is abusing its position of power as a provider of some educational services to profit off of students’ data when they use other Google services—services that Google has arbitrarily decided don’t deserve any protection. Of course, that’s not to say that Google’s use of Chrome Sync data for non-educational purposes isn’t a problem. While we agree that Chrome Sync is an incredibly useful service, we don’t think students should be guinea pigs in Google’s efforts to improve its products without explicit parental opt-in—even if their data is anonymized and aggregated. The Student Privacy Pledge website clearly says that service providers will “use data for authorized education purposes only”—and anonymized or not, using Chrome Sync data for anything other than the Chrome Sync service itself does not constitute an educational purpose. While our FTC complaint is focused on Google, rest assured that we’re not limiting our campaign to one company. In the coming weeks and months we intend to continue investigating the practices of other cloud-based education services. And if you’re a parent or teacher with first-hand knowledge of other cloud-based education services, you can help us out by filling out our survey so we can gather more information to decide where to focus next! Yesterday, in Backpage.com v. Dart a unanimous panel of the Seventh Circuit Court of Appeals in a lively opinion ordered Thomas Dart, the sheriff of Cook County, Illinois, to end his “campaign of suffocation” against the website and stop violating its First Amendment rights. The court of appeals rejected Sheriff Dart’s contention that he was merely expressing his personal distaste for Backpage and not using his position as a government official to coerce Visa and MasterCard into discontinuing business with the website. Rather, the court of appeals held that Sheriff Dart’s actions amounted to an unconstitutional prior restraint on Backpage’s speech. As we wrote about last month, we submitted an amicus brief to the Seventh Circuit arguing that government officials such as Sheriff Dart may not use their positions of authority to coerce companies with express or implied threats of legal liability into taking actions that censor speech—whether online or offline. The Seventh Circuit agreed. Overruling the district court that had denied Backpage’s request for a preliminary injunction, the court of appeals issued the following order: Sheriff Dart, his office, and all employees, agents, or others who are acting or have acted for or on behalf of him, shall take no actions, formal or informal, to coerce or threaten credit card companies, processors, financial institutions, or other third parties with sanctions intended to ban credit card or other financial services from being provided to Backpage.com. While Sheriff Dart is rightly concerned about sex trafficking, the court of appeals noted that no one is claiming that there is “no constitutionally protected speech in the ads on Backpage’s website.” (emphasis in original) Yet “Visa and MasterCard bowed to pressure from Sheriff Dart and others by refusing to process transactions in which their credit cards are used to purchase any ads on Backpage, even those that advertise indisputably legal activities.” (emphasis in original) Sheriff Dart had written letters “intimating that the credit card companies could be prosecuted for processing payments made by purchasers of the ads on Backpage that promote unlawful sexual activity, such as prostitution.” The court of appeals noted that “It was within days of receiving the letter that the credit card companies broke with Backpage. The causality is obvious.” Thus the court held that Sheriff Dart’s actions constituted a prior restraint in violation of the First Amendment. The Seventh Circuit equated Sheriff Dart’s campaign of “depriving the company of ad revenues by scaring off its payments-service providers” rather than going after Backpage directly through litigation with “killing a person by cutting off his oxygen supply rather than by shooting him.” As we stated in our amicus brief, targeting financial intermediaries, though indirect, can be a particularly effective way of censoring online speech because “certain financial intermediaries play a near-existential role in online expression.” Like access to Internet connectivity, access to the financial system is a necessary precondition for the operations of nearly every other Internet intermediary, including content hosts and platforms. The structure of the electronic payment economy . . . make these payment systems a natural choke point for controlling online content. The Seventh Circuit made clear that Sheriff Dart, in his official capacity, does have “freedom of government speech.” However, the court of appeals stressed that such freedom has limits. He or any other government official or entity “is not permitted to employ threats to squelch the free speech of private citizens.” Following the Senate’s September hearing, the House Judiciary Committee today held a hearing on reforming the Electronic Communications Privacy Act, the federal law that regulates government access to private communications records stored by online service providers. Congress is considering a pair of identical bills that would create a warrant requirement for any government entity that seeks personal content stored in the cloud: the Senate version is the Electronic Communications Privacy Act Amendments Act (S. 356) while the House calls theirs the Email Privacy Act (H.R. 699). Not only did several of the members attending the House hearing express support for H.R. 699, the bill now has over 300 co-sponsors and would overwhelmingly pass if brought up for a vote on the House floor. However, the Securities and Exchange Commission is once again asking for more authority than it has today—a proposal that unfortunately Chairman Goodlatte (R-VA) supports. ECPA as currently worded requires the government to obtain a search warrant based on probable cause, and signed by a judge, before it can access communications content such as emails or text messages that are 180 days old or newer. The law on its face also permits agencies to use administrative subpoenas to access communications content older than 180 days. However, the majority of service providers follow the Sixth Circuit’s ruling in Warshak and have for the last few years required that the government show up with a warrant before turning over any content stored in the cloud, regardless of age. Given that the SEC and other civil agencies lack warrant power (which is reserved for law enforcement), they are eager for a new authority to obtain any communications content should Warshak’s across-the-board warrant requirement become codified. The SEC testified that currently it does not use administrative subpoenas to obtain communications content from online service providers, and instead seeks emails directly from individuals. Yet the agency wants to be able to obtain not only older communications content from third parties, but also messages that are 180 days old or newer, which is authority that civil agencies currently do not have in any form—a point that Rep. Sensenbrenner (R-WI) made. On this issue, Rep. DelBene (D-WA) expressed concern that the SEC seems to view online service providers as witnesses or informants at the disposal of agency investigators, rather than the digital homes of personal communications. She likened the SEC’s position as demanding the authority to order a locksmith to unlock an individual’s front door anytime the agency seeks information in an investigation. Taking a global view, Rep. Lofgren (D-CA), a 2014 EFF Pioneer Award winner, made the point that ECPA without reform is bad for American businesses as potential customers in foreign countries view personal content stored in the cloud as highly vulnerable to U.S. government intrusion. EFF continues to support a “clean” ECPA reform bill that protects personal content stored in the cloud the same way the Fourth Amendment clearly protects personal content stored in a home or office—with a probable cause warrant. We urge you to contact your senators and representatives today! The official release of the text of the Trans-Pacific Partnership (TPP) on November 5 not only confirmed our fears about how it would threaten our rights online and over our digital devices, but also kicked off a 90-day countdown to President Obama signing the deal. A few days later, the White House formally requested the International Trade Commission (ITC) to begin its study of the impacts of the TPP on the U.S. economy for a report to come out in the Spring before Congress approves (or ratifies) the deal—which as explained below, is a separate and later step to signature of the deal. Now that the TPP's approval and ratification in the United States is on the horizon, here's what you need to know about what's going on and what's to come. This state of play will be useful as we work to defend our digital rights against the largest trade deal in history. The series of procedural requirements imposed by the Fast Track bill that passed this summer gives us a useful timeframe to know what's coming next. But this clarity still doesn't make up for the loss of congressional oversight that we would have had if that bill had not passed. What Congress gave up with Fast Track was the ability to challenge any of the thousands of provisions that had been decided behind closed doors. So after February 4, 2016, the President will be able to unilaterally sign the TPP for the U.S. Thankfully, there's one more hoop the TPP must jump through before it becomes binding on the United States. Both congressional houses must ratify the agreement in the form of approving "implementing legislation" that the White House will submit to lawmakers. This submission will happen after the President's signature, likely sometime in April or May. Once that happens, the House has 60 days from the bill's introduction to hold a vote on it and the Senate gets another 30 days, so 90 days in total, to approve or reject it. Since this second timeline only begins when the White House decides that they're ready for it, it all rests on whether the executive branch believes that it has the votes to get it through both houses. That's why it's critical that we call on our lawmakers to come out against this agreement: because that's how we can stop it. President Obama only needs a simple majority to get TPP approved in both houses, so the Administration has been wasting no time convincing members of Congress to support it. They've been in lobbying overdrive since they concluded negotiations in the end of October even before the text was released. While some lawmakers have already come out in clear support or opposition to the deal, most say they'll weigh their decision after reading the actual contents of the more than 6,000 page text—though it's difficult to believe that many of them will actually do this. One of the most vocal TPP proponents, Sen. Orrin Hatch, has been critical of the deal and has even demanded that the White House go back to the negotiating table. But he's not holding back support because he's worried about the intellectual property provisions. It's quite the opposite: as a longtime shill for the pharmaceutical industry, he thinks that the deal doesn't go far enough to secure corporate interests and is demanding that the TPP enforce even lengthier monopolies on medicines (although he doesn't seem to have a problem with the TPP's copyright provisions, which the MPAA has already fully supported [pdf]). Meanwhile Congress members who backed Fast Track continue to receive special treatment from top Obama administration officials. They are getting invitations to join cabinet officials on their first trips to Cuba and the President himself is bending over backwards to please the House Democrats who voted in favor of the trade bill this summer so they stay by his side for the TPP implementation vote. Thankfully, none of the Democrats who voted against Fast Track have stepped forward to support the deal so far. U.S. Trade Representative Michael Froman, for example, personally visited Long Beach, California to win over Rep. Alan Lowenthal, who voted against Fast Track this summer.  Major TPP champion Rep. Paul Ryan gave up his role as the Chairman of the Ways and Means Committee, where the TPP vote would initially be held in the House, to become the Speaker of the House. Unfortunately for us, his replacement is likely to be just as gung-ho about the TPP as Rep. Ryan. Rep. Kevin Brady, a long time supporter of trade agreements with toxic digital policies, now sits as the Committee's Chairman, with Rep. Dave Reichert as the Chairman of its subcommittee on Trade. Despite dubious claims of ambivalence about his position on TPP, these new Republican House leaders seem to pretty poised to bring the TPP to congressional ratification. Rep. Reichert, for one, claims that his experience in law enforcement and a hostage negotiator has prepared him to lead the trade debate because he knows "when to kick in the door.” For now, there is no clear reading on how Congress will vote on the TPP's implementing legislation, but it's going to be a close vote. Our strategy will be two-fold: Fighting the TPP is as much about defending our digital rights as it is about rejecting opaque, corporate-driven policymaking. Internet users in all TPP countries can take action to call on their representatives to reject this agreement, but stopping this deal in the U.S., will effectively also defeat it in the 11 other countries. If we want to ensure that laws don't just uphold powerful private interests, but are designed and implemented with the public's best interests in mind, then we must stop the TPP—for the sake of the Internet, our rights, and our future.  Katherine W. was seven years old, in the third grade, when her teacher first issued Google Chromebooks to the class. Katherine’s father, Jeff, was concerned. It wasn’t because he had a problem with technology. In fact, Jeff and his family are technology enthusiasts. “We bought a house in this area primarily because of the school district. And one of the things that excited us about the school was the use of technology,” Jeff explained during a recent interview with EFF. That enthusiasm waned when the school retired its former laptops and brought in Chromebooks for the students instead, also assigning each third grader a profile in Google Apps for Education, Google’s cloud-based education suite. Chromebooks may have been cheaper, but Jeff feared they might come at the cost of his daughter’s privacy.  Roseville City School District, near Sacramento, is one of the many districts now issuing mobile digital devices to students. In fact, one study estimates that nearly a third of middle and high school students in the United States are using mobile devices like laptops and tablets issued by their schools.   When Jeff learned about the Chromebooks being offered to third graders, he acted quickly and was able to negotiate with his daughter’s teacher so she could use a different computer and not have to use a Google account. But as third grade came to a close, Roseville City School District made clear that there would be no exception made the next year. The Family Educational Rights and Privacy Act (FERPA) is a federal law that protects students’ “educational records,” including personally identifiable information. The data that students often use to log into a Chromebook or Google Apps for Education—like name, student number, and birthday—is covered by FERPA. Under FERPA, this data generally can’t be shared with third parties—including Google—without written parental consent. But Roseville City School District never sought written consent from Jeff or his wife. The district provided no details about the types of devices students would be required to use or the data that would be collected on students. Rather than allowing Jeff to sign his daughter up for the Chromebook program, the district consented on his behalf, making the device mandatory for Katherine—with no ability to opt out. Many people—including Jeff—assumed that the law would prevent Google from collecting data on his daughter for advertising purposes. But the truth is more complicated. While Google is legally forbidden from creating a profile on Katherine when she’s using the school-sanctioned Google Apps for Education tools (which include email and document sharing), it can collect data as soon as she uses other Google services that aren’t part of the student-specific suite—including YouTube. This means that Katherine is required by the school to use Google with a personalized Google Account, and Google can create a profile of her and use it for advertising purposes the moment she clicks away from the Google Apps for Education suite. But it’s even worse. When schools issue Chromebooks, Google’s browser Chrome comes with Chrome Sync turned on by default. So instead of storing sensitive data—like browsing history—locally on the device, Chrome syncs that data to the cloud and allows Google to collect and indefinitely store sensitive data about students’ use of Chrome to browse the Web. With EFF’s guidance, Jeff started a dialogue with the Roseville City School District over the summer to try to resolve the issue before his daughter started fourth grade. He emailed the district superintendent, the principal, and the technology director, outlining his concerns. Jeff even offered to buy a different laptop for his daughter, but the school refused. Finally, after several emails and a tense meeting later, the district agreed to provide Jeff’s daughter with a non-Google option for fourth grade—but once again declared that such an accommodation would not be possible for fifth grade. That’s when EFF reached out to the district. Our legal team drafted a letter to the Roseville City School District to outline the privacy concerns associated with school-issued Chromebooks. The letter urged the district to permit “all students – if their parents so decide – to use alternative devices, software and websites, for the upcoming school year and every year.” The district refused to meet with us to discuss the issue. For Jeff, the biggest concern isn’t just the data Google collects on students. It’s the long-term ramifications for children who are taught to hand over data to Google without question. It’s normalizing the next generation to a digital world that’s less private by default, and built on proprietary software. As Jeff explained it, “In the end, Google is an advertising company. They sell ads, they track information on folks. And we’re not comfortable with our daughter getting forced into that at such an early age, when she doesn’t know any better.” Learn more about the privacy problems of school-issued digital devices. Are you a parent, teacher, or school administrator concerned with the privacy risk of school-issued devices? Tell us about it. Image courtesy Jeff W. If your child's school issued them a Chromebook, there are some important settings you can chance to improve their privacy. Be sure to also check out our Guide to Google Account Privacy Settings for Students. Open the Chromebook’s settings by clicking on your username in the bottom right-hand corner, then clicking “Settings.” Scroll all the way to the bottom, and click "Show advanced Settings..."  Click the “Advanced sync settings…” button. Then from the dropdown menu, select “Choose what to sync” and then un-check all the boxes below. Then click “OK”.  Next, under “Privacy”, click the "Content settings..." button.  Once you have finished, click “Done” in the bottom right, and you can then close the Settings window. Finally, if your school allows it, we highly recommend installing Privacy Badger (which will help block non-consensual third-party tracking) and HTTPS Everywhere (which will help encrypt your child's web browsing) in your child's Chromebook. And don't forget to also check out our Guide to Google Account Privacy Settings for Students. Do you know about a school issuing digital devices to students?


   Even if your student doesn't use a Chromebook, they may have a Google Apps for Education (GAFE) account, which is essentially a Google account provided by the school. GAFE accounts can be accessed from any browser, and the privacy settings affect your student regardless of which browser they use to log in to GAFE. In order to maximize your child's privacy, we advise changing the settings below. If your child's school issued them a Chromebook, be sure to also check out our Guide to Chromebook Privacy Settings for Students. Using any web browser, open the URL https://myaccount.google.com/privacy and make sure you are signed in to your Google account. Then, click "Activity controls" on the left, under "Personal info & privacy". Then, make sure all the slider/toggles are to the left ("paused"). Additionally, under "Your searches and browsing activity", uncheck "Include history from Chrome and other apps in your Web & App Activity."  Click  "Ads settings" on the left, and then click "Manage Ad Settings"  Then, make sure "Ads based on your interests" is toggled to the left ("Off")  Congratulations! You've successfully enabled the most privacy-protective settings in your child's GAFE account! And don't forget, if your child's school issued them a Chromebook, be sure to also check out our Guide to Chromebook Privacy Settings for Students. Do you know about a school issuing digital devices to students?


   Coders have never been more important to the security of the Internet. By identifying and disclosing vulnerabilities, coders are able to improve security for every person who depends on information systems for their daily life and work. Yet this week, the South African's Department of Justice and Constitutional Development is closing its open invitation to comment on a vague and sweeping draft computer crime bill that threatens to create legal woes for security researchers who expose security flaws—and in addition, create disproportionate new penalties for online hate speech and copyright infringement. EFF has provided expert opinion on the draft text of the Cybercrimes and Cybersecurity Bill. In our submission (PDF), EFF opposed the wholesale criminalization of software and hardware tools that can be used to simulate attacks or demonstrate a particular vulnerability against computers, devices or electronic communication networks. While they can be used for malicious purposes, they are also crucial for research and testing, including for "defensive" security efforts to make systems stronger and to prevent and deter attacks. The draft, as currently written, might severely curtail commercial “penetration testing” firms that are critical for the modern economy, academic scholarship, legitimate security research, and other activities that benefit society. EFF told the South African Justice Department that the draft bill must not criminalize the creation, possession and distribution of tools that are fundamentally designed for the purpose of carrying out an attack. These tools have legitimate, socially desirable uses, such as identifying a practical vulnerability. Another major problem with the draft bill is its impact on the ability of coders to access computers, devices or electronic communication networks for security testing without explicit permission. Examining computers without the explicit permission of the owner is necessary for a vast amount of useful research, which might never be done if permission were required. If the South African government moves to enact this clause, researchers who study others’ systems in the course of good faith for legitimate research (including to test the security of their own data) may become criminals. The proposed text should affirmatively protect access for purposes of security testing even if the security researcher does not have a written or oral authority to access the system. Clause 2 of the proposed text is too broad and vague regarding what constitutes an offense. The proposed Bill should affirmatively protect good faith activities that are need for security testing even if the security researcher does not have a “written authority" to access the system. It's very important that criminal law be precise on what constitute both unlawful and criminal malicious intent (mens rea). The proposed Cybercrimes and Cybersecurity Bill also unfairly imposes harsher penalties for the dissemination of hate speech when it occurs online. South Africa's Promotion of Equality and Prevention of Unfair Discrimination Act (2000) already excludes hate speech from free expression, prohibiting the publication, communication, or other dissemination of speech that could be reasonably construed to demonstrate a clear intention to 1) be hurtful 2) be harmful or to incite harm or 3) promote or propagate hatred. According to the 2000 Act, cases involving hate speech are to be referred to the equality court, which is authorized to hold an inquiry and make an "appropriate order." The Act lists a number of possible orders which the court could make, ranging from an "unconditional apology" to payment of damages. South African common law also provides for penalties for "crimen injuria," or "unlawfully and intentionally impairing the dignity or privacy of another person." The proposed Cybercrimes and Cybersecurity Bill would automatically impose criminal penalties (of a fine and/or up to two years imprisonment) for the unlawful and intentional distribution of any "data message which advocates, promotes or incites hate, discrimination or violence." This treatment of online speech as inherently separate from speech which takes place offline. The imposition of harsher penalties for online speech has become common as states attempt to grapple with the expansion of the public sphere into cyberspace. In the aftermath of the horrific attacks on Charlie Hebdo this past January, France proposed an anti-terrorism bill that would provide for harsher penalties for the glorification of terrorism if it took place online. Similarly, the rationale that online speech is inherently more dangerous has been used to push for laws restricting anonymous online speech in a range of countries. The criminalization of demonstrating or testing for vulnerabilities gives vendors of flawed products the ability to deny the existence of those flaws, even months or years after those flaws have been discovered, or to wrongly suggest that the vulnerabilities are merely theoretical. This will put the personal information of many South Africans at risk. We need to fix this. The provision also provides vendors with enhanced legal leverage to frighten researchers into silence. This will harm the public by allowing insecure and broken technology to remain unpatched and be used, sometimes by millions of people. The wording of the bill must not criminalize the legitimate activities and use of tools needed for independent security research, academic study, and other good-faith activities that serve the public interest and ultimately make the public more safe. These problems are compounded by two provisions that treat online offenses more harshly than similar offenses committed off-line; the automatic criminalization of online hate speech covered above, and a provision that would criminalize many non-commercial acts of copyright infringement that we covered earlier in Deeplinks. We must be wary of any attempt to impose additional penalties on speech due purely to the medium through which it takes place. South Africa's Right to Know coalition has just launched a Change.org petition asking for the Cybercrimes and Cybersecurity Bill to be withdrawn. Unless the significant concerns that we have expressed above can be addressed before the draft becomes law, we agree that its withdrawal is the best option for South Africa. No posts found The Prevention of Electronic Crimes Bill (PECB) has received harsh criticism inside and outside of Pakistan since its radical re-drafting in April of this year. A coalition of Pakistan’s leading online rights groups and businesses warned the current version, written with no input from legal experts or technologists, would “adversely impact the IT industry…. and [the] constitutional rights and safeguards guaranteed to citizens”. Human Rights Watch went further, saying it constitutes “clear and present danger to human rights”. But it took one of Pakistan’s leading legal experts on computer crime jurisprudence, Zahid Jamil, to call the bill “by far the worst piece of cybercrime legislation in the world.” A close look at the latest text, due to be presented for a vote in Pakistan’s National Assembly soon, demonstrates just why the bill has everyone so worried. Originally introduced in January 2015, the PECB was ostensibly intended to combat common digital threats such as fraud, online stalking, and harassment. Instead, through a combination of poor drafting and politicized additions, it has ended up as a sweeping and arbitrary mess of law: criminalizing free expression and innocent technology uses, while handing a largely unsupervised set of censorship and surveillance powers to the Pakistani authorities. Here’s our rundown of the very worst of a terrible bill. Writing new law explicitly targeting crimes that are committed with new technology is notoriously challenging. A lack of technical knowledge tempts lawmakers to use vague language that sweeps up innocent or commonplace behaviour, and leads them to invent drastic new punishments for offences that are already covered by existing statutes. Zealous prosecutors then use these weaknesses to unfairly pursue the innocent, while courts impose unreasonable punishments for what would otherwise be minor offenses or perfectly harmless behaviour.  In the United States, for example, the vagueness and breadth of the Computer Fraud and Abuse Act (CFAA) has led to some prosecutors defining as crimes acts that are merely violations of a website’s terms of service. Some U.S. courts have rejected this interpretation, but prosecutions along these lines continue to waste police and court time and devastate innocent users’ lives­—including that of Aaron Swartz, the prominent Internet writer, coder and activist, whose prosecution under the CFAA preceded his suicide. (EFF has joined lawmakers and other groups in advocating for Aaron’s Law, which would rein back these interpretations of the CFAA in the United States.)  Astoundingly, Pakistan has doubled down on the CFAA’s mistakes. Sections 3 and 4 of the bill include CFAA-style language, stating that anyone who intentionally gains unauthorised access to any information system or data, or copies or otherwise transmits or causes to be transmitted any data, will be punished with three year’s imprisonment or a heavy fine. The bill’s definition of “unauthorised access” (2(1)(dd)) includes language that could easily be interpreted as including violations of terms of service or conditions (it specifically includes access “in violation of the terms and conditions of the authorisation”). Other definitions are equally broad. An “information system” is defined earlier in the bill as “[any] electronic system for creating, generating, sending, receiving, storing, reproducing, displaying, recording or processing any information”, and “information” is defined in Section 2 (1) (r) as including “text, message, data, voice, sound, database, video, signals, software, computer programs, any form of intelligence […] and codes including object code and source code.” Between these two broad definitions, an information system might be anything at all. The bill includes no language to limit prosecutions to those who gain unauthorised access for malicious reasons, and no exemptions for those who might transmit “unauthorised” data in the public interest, such as whistle-blowers and journalists. Regular Internet users could be caught up in Section 3 and 4’s definitions too. As Article 19 and Pakistan’s Digital Rights Foundation have noted, these sections could be used to target and prosecute any Internet user accessing a government-censored Web page via a VPN or Tor.  Despite nearly thirty years of evidence of the damage of overly broad computer abuse law, the PECB authors have managed to extend its reach even further into innocent and positive endeavours. If PECB passes without reform, Pakistan can expect to see more unjust prosecutions and persecution of innocent citizens. As with the CFAA, security researchers and technically-sophisticated Internet and mobile phone users are particularly at risk from the PECB. Section 16 says that whoever changes, alters, tampers with or re-programs the unique device identifier of any communication equipment and starts using or marketing such device for transmitting and receiving “information” can be jailed for up to three years, a fine up to one million rupees (around 9,500 USD) or both. The provision is overly broad and targets those who use such tools for legitimate purposes. Changing the WiFi MAC address on your laptop to limit tracking should not be a crime. Under the PECB, it would be.  Section 17 prohibits any “unauthorised interception” of “electromagnetic emissions from an information system that are carrying data,” essentially criminalizing much of the practice of modern radio hams: not to mention transforming scanning for open wifi into a jailable offence. Section 20 creates a crime, punishable by two years in prison, of writing or distributing “malicious code.” While this section manages to contain a requirement of harmful intent, its unclear scope could lead to a chilling effect on legitimate security research by Pakistan nationals. Section 23 invents a new offence of “spoofing,” which involves “dishonestly establish[ing] a website or send[ing] any information with a counterfeit source.” There are almost no limits on this vague definition—but if you “spoof” in Pakistan, you could be facing three years in jail, or a fine of 500,000 rupees (4,750 USD). It’s not just high tech crime that PECB targets, however. Section 9 and 10 together pose a serious threat to free expression to everyone online. Section 9 states that anyone who “prepares or disseminates information, through any information system or device” with the intent to “glorify an offence or the person accused or convicted of a crime and support terrorism or activities of proscribed organizations” and “advance religious, ethnic or sectarian hatred” shall be punished with imprisonment up to five years, a fine up to ten million rupees (around 95,000 USD) – or both. In a note below the provisions, glorification is defined as “any form of praise or celebration in a desirable manner.”. The former UN Special rapporteur on Freedom of Expression Frank LaRue has previously noted that the term “glorification” fails to meet international human rights standards; the PECB’s vague definition does nothing to lift the PECB over this bar. Section 9 could easily be used, for example, to target lawyers discussing the merits of a case or the legality of charges against an accused client. The punishment for Section 9’s glorification is severe (five years imprisonment, or 95,000 USD), but grows even more harsh in Section 10, where glorification is coupled with what the bill describes as “cyber-terrorism” – the intent to “coerce, intimidate, overawe or create a sense of fear, panic or insecurity in the Government or the public or a section of the public or community or sect or create a sense of fear or insecurity in society” or “advance religious, ethnic or sectarian discord”. “Cyber-terrorism,” can lead to imprisonment for up to fourteen years, a fine up to fifty million rupees (around 474,000 USD) – or both. Many countries have fought laws and directives designed to force service providers to log the actions of their users: Pakistan’s lawmakers have chosen to smuggle it in with a single clause. Section 29 requires internet service providers (ISPs) to retain all “traffic data” for a minimum period of one year, or any period of time that the Pakistan Telecommunication Authority requests, and “provide that data to the investigation agency or the authorised officer whenever so required.” This means that all personal information and communications of individuals residing within the borders of Pakistan will be retained for at least one year, and may be shared with any investigation authority— including, as we will see, those of foreign governments. But the PECB goes even further than requiring data retention. Section 28 states that if an “authorised officer” is satisfied that any data is “reasonably required” for the purposes of a criminal investigation and that there is a risk or vulnerability that the data may be “modified, lost, destroyed or rendered inaccessible,” then the officer may require the person in control of the data to hand it over or ensure that it be preserved for a period not exceeding ninety days. Sub-section (2) goes on to stipulate that the officer can request the court to extend the 90-day period without limit.  The provision does not define what “reasonably required” might mean. No warrant or judicial authorisation is required with a Section 28 order: in effect, the authorised officer decides for him or herself what is reasonable, and what is required. The bill does suggest (but does not mandate) that the officer notify a court of the acquisition after the deed has been done. Even with notice, there is no provision made for the court to consider the merits of the officer’s actions, and no procedural safeguards or guidelines as to how and whether the officer could obtain the information. Section 32 (1) (g) also grants authorised officers the power to require any person in possession of decryption information of “an information system, device or data under investigation” to hand over decryption information necessary to decrypt any data required for the investigation. This means that regular users would be required to hand over their own decryption keys, or face prosecution. As with other requirements to decrypt data, it’s unclear how an individual can show that they do not possess the key or access to the unencrypted data. Section 30 allows an “authorised officer” to apply for a warrant with the court to “enter the specified place,” “search the premises and any information system, data, device or storage” and “access, seize or similarly secure” the data. There are several problems with this provision, which seemingly attempts to place a check on data acquisition. First, the warrant provision is not mentioned in the rest of the bill, rendering it an empty shell in procedural protection. Second, the threshold for obtaining a warrant is dangerously low – an officer need only show that the data is “reasonably required for the purpose of a criminal investigation”. There is no defined legal standard to ascertain what is reasonable and what is unreasonable. Third, and again with ambiguous language, there is no clear definition of “seize”; the law fails to outline any sort of mechanism or process for how the data is retained, copied, or taken possession of. Section 34 grants the Pakistan Telecommunication Authority the power to “manage information” online and remove and/or block content on the web. The information subject to censorship includes text, messages, data, voice, sound, database, or video. Content may be censored if “necessary in the interest of the glory of Islam or the integrity, security or defence of Pakistan or any part thereof, friendly relations with foreign states, public order, decency or morality, or in relation to contempt of court or commission of or incitement to an offence” in the bill itself. Excessively sweeping, Section 34 enables the government to censor any content available through any electronic device (including video game consoles, phones, and anything that might connect to the Internet) that it considers unacceptable. There is no requirement for the Pakistan Telecommunication Authority (PTA) to obtain court approval before ordering ISPs to censor content. Sub-section (3) peripherally mentions that the PTA has the power to consider complaints, it does not outline any appeal mechanism for any rejected complaints that it handles. Section 36 authorises a court to order ISPs to “collect or record such information in real-time in coordination with the investigation agency” when “reasonably required” for the purpose of a criminal investigation. The live data collection can initially only be authorised for a total of seven days, but the period can be extended with court approval with no time limit. This means that an individual could be placed under indefinite, perpetual surveillance. The section does not provide any guidance as to what grounds the court should consider when considering a time extension.  Sub-section (4) also states that a court may require the agency to “keep confidential” the existence of the investigation, or the execution of real-time data collection and any information relating to it. In short, the provision allows for an investigation agency to order ISPs to perpetually track individuals and requires them to deny that they are doing so. Finally, Section 38 (2) permits the Pakistani government to unilaterally forward to a “foreign government, 24x7 network, any foreign agency or any international agency or organization” any information that it obtains from its own investigations through this bill. The government need only consider that the disclosure of such information “might assist” the other entity. There is, as Privacy International notes in their legal analysis, no oversight mechanism for this sharing of sensitive and personal data with foreign governments and spy agencies. It’s hard to imagine how Pakistan could have sabotaged more of its digital future in the fourteen pages of the Prevention of Electronic Crimes Bill. The legislation is a grab-bag assortment of abusive provisions that violate the most basic of human rights. Through censorship, surveillance, and the stifling of free speech, the Prevention of Electronic Crimes Bill gives new meaning to the word draconian. If you’re a Pakistani voter, tell your legislators to oppose PECB now. There’s not much time to prevent the “worst piece of cybercrime legislation in the world” from becoming law. The NSA’s collection en masse of the call detail records of millions of ordinary Americans ended quietly at midnight November 29. The bulk collection was phased out after a 180-day transition period provided for in the USA FREEDOM Act. The USA FREEDOM Act was signed by President Obama on June 2, 2015, and amended section 215 of the USA PATRIOT Act, which the NSA had claimed gave it authority to collect the records, to specifically prohibit the bulk collection. We wrote previously on what that means for the collection of records and how it affects our cases against the NSA. Rather than the previous mass collection of all phone records possessed by particular providers, the USA Freedom Act limits collection of call detail records to instances where there is "reasonable, articulable suspicion" that a specific "individual, account, or personal device" is associated with international terrorism. This "individual, account, or personal device" is known in the law as  a “specific selection term.” Rather than the old method whereby the NSA collected record of all customers of certain providers, the NSA, under this program, now obtains only those records associated with the specific selection term. The new procedure still allows the NSA to perform “bulky,” if not “bulk,” collection because providers are required to turn over not only the records associated with the specific selection term, but also the records of all those who called or were called by the specific selection term. Note also that USA Freedom has a broader definition of specific selection term for the collection of tangible things other than calling records. In these cases, the term must be a "person, account, address, or personal device." The language of the legislation directs that this term be used to limit the scope of these requests for information to the greatest extent possible, and specifies that it cannot be used to collect records that are overly broad, like those that relate to a geographic region or an entire electronic communications service provider. The government also recently complied with another important requirement of USA FREEDOM. On November 25, the Foreign Intelligence Surveillance Court designated five individuals as eligible to serve as amicus curiae, that is, "friends of the court." This was perhaps the most important reform of USA FREEDOM. Previously, the FISC almost always heard only from the government in deciding whether to grant the government's requests to conduct surveillance. As we noted when USA Freedom passed, we don’t think it’s perfect, but it is a step in the right direction. USA FREEDOM and the phase-out of the old mass collection of call records does not end un-warranted bulk surveillance. It is still going on with respect to other communciations media and under the alleged authority of other laws.  We continue to challenge the collection being conducted under Section 702 of the FISA Amendments Act and Executive Order 12333 next. Under 702, the government has siphoned communications directly from tech companies and the key infrastructure of the Internet and worse. Executive Order 12333 is supposed to protect Americans from Presidentially-directed spying; however, despite the protections, it it is being used for mass spying that collects Americans' communications, address books, and other information. And we still don't know how it has been used to target non-US persons, despite the requirement under international law that surveillance be necessary and proportionate. So as we mark a milestone in section 215 reform, we call again for substantial reform of 702 and 12333. And as a first step in that reform, the government must make the details of programs conducted under these authorities public.  What if, in response to the terrorist attacks in Paris, or cybersecurity attacks on companies and government agencies, the FBI had come to the American people and said: In order to keep you safe, we need you to remove all the locks on your doors and windows and replace them with weaker ones. It's because, if you were a terrorist and we needed to get to your house, your locks might slow us down or block us entirely.  So Americans, remove your locks! And American companies: stop making good locks! We'd all reject this as a bad idea. We'd see that it would make us all vulnerable, not just to terrorists but to ordinary thieves and bad guys. We'd reject undermining our daily security in favor of a vague potential that in some cases, law enforcement would be guaranteed, quick, easy access to our homes. We'd say to the FBI: Stop right there. We need more security in the wake of these attacks, not less. Yet that same tradeoff is similar to what's being asked of us in the attacks on strong encryption. The FBI isn't technically asking for no locks—it's asking for weakened ones so that it can guarantee that it can break any lock that we buy or use—but the end result is the same. We're made more vulnerable. As with the locks on our doors, digital locks can't be made to allow access to all the good guys and none of the bad guys. The lock can't tell the difference, and even more vulnerabilities are created by building complicated processes for storing digital keys, as demonstrated by a recent MIT report and an open letter to David Cameron by Harvard Professor (and EFF Board member) Jonathan Zittrain. Right now the FBI's strategy is focused on putting pressure on companies like Apple, Microsoft and Google, to prevent us from ever getting access to good locks in the first place. Yet if the FBI was publicly calling for home builders and locksmiths to stop offering you the strongest possible home or office security systems, we'd see the folly of their strategy outright. EFF and many others have long demonstrated that limiting our access to strong encryption is a bad idea. But somehow, maybe because the way these locks work is more hidden from users in the context of digital networks and tools, the argument continues to be raised by an FBI that should know better. And by politicians who should know better, too, like Hillary Clinton.  The response to to insecure networks and digital technologies must be to make them stronger. And yet this basic message is not only lost on those who call for encryption controls, but it has also been undermined by the cybersecurity approach of CISA, which instead of encouraging better security by those who store our information, pushes companies to increase the risks we already face by "sharing" more of our data with the government. Of course, the lapses in government security are already well documented. The same wrongheaded approach is on display when our Congress fails to reform the Computer Fraud and Abuse Act to protect the security researchers whose work results in better protections for us all—and instead pushes for a worse version of the law, with a still broader scope and harsher penalties. Unlocking everyone's doors isn't the answer to global crime or terrorism. Building and supporting stronger security is. FCC will not seek to ban free software from wireless routers, according to a clarification it made earlier this month on a rulemaking related to radio devices. An earlier draft of the official proposal included a specific reference to device manufacturers restricting installation of the open-source project DD-WRT. That line, in the context of the larger proposal, created confusion in a community of router hackers that already operate in an often unwelcoming environment. Router makers rarely provide much in the way of support or documentation to people developing new software, and have a bad record on delivering software updates to end users. Against this background, the idea that regulators might require or urge those manufacturers to take proactive steps against third-party developers was cause for alarm. This is especially true considering the valuable innovations and developments that have come out of the third-party router software community—innovations like advancing the state of the art in mesh networking and combatting slowdowns that come from "bufferbloat." Beyond that, free router software is frequently more secure than the manufacturer option, because it continues to receive patches and critical updates through community support. EFF was far from alone in its concern about the possibility of a regulatory crackdown on free router software. Working together with the Save Wifi coalition, we re-launched our "Dear FCC" platform, originally developed to help the public provide comment on the net neutrality rulemaking earlier this year. More than 1000 concerned individuals used the platform to leave comments on this more recent rulemaking, making it one of the most active open FCC dockets. To its credit, the FCC seemed to get the message loud and clear. In a blog post earlier this month title "Clearing the Air on Wi-Fi Software Updates," the chief of the agency's Office of Engineering & Technology explained the situation: [T]here is concern that our proposed rules could have the unintended consequence of causing manufacturers to “lock down” their devices and prevent all software modifications, including those impacting security vulnerabilities and other changes on which users rely. Eliciting this kind of feedback is the very reason that we sought comment in an NPRM and we are pleased to have received the feedback that will inform our decision-making on this matter. In my last post I recognized the need to work with stakeholders – particularly the user community – to address these concerns in a way that still enables the Commission to execute its mandate to protect users from harmful interference. I’m happy to say that the OET staff and I have spoken directly with some of these stakeholders in the last few weeks. One immediate outcome of this ongoing dialogue is a step we’ve taken to clarify our guidance on rules the Commission adopted last year in the U-NII proceeding. Our original lab guidance document released pursuant to that Order asked manufacturers to explain “how [its] device is protected from ‘flashing’ and the installation of third-party firmware such as DD-WRT”. This particular question prompted a fair bit of confusion – were we mandating wholesale blocking of Open Source firmware modifications? We were not, but we agree that the guidance we provide to manufacturers must be crystal-clear to avoid confusion. So, today we released a revision to that guidance to clarify that our instructions were narrowly-focused on modifications that would take a device out of compliance. That revision is a welcome one. We'll continue to monitor the progress of this proposed rule to ensure it can't be used to jeopardize the important role that free third-party software continues to play in the router ecosystem. Earlier this year it was revealed that Lenovo was shipping computers preloaded with software called Superfish, which installed its own HTTPS root certificate on affected computers. That in and of itself wouldn't be so bad, except Superfish's certificates all used the same private key. That meant all the affected computers were vulnerable to a “man in the middle” attack in which an attacker could use that private key to eavesdrop on users' encrypted connections to websites, and even impersonate other websites. Now it appears that Dell has done the same thing [PDF], shipping laptops pre-installed with an HTTPS root certificate issued by Dell, known as eDellRoot. The certificate could allow malicious software or an attacker to impersonate Google, your bank, or any other website. It could also allow an attacker to install malicious code that has a valid signature, bypassing Windows security controls. The security team for the Chrome browser appears to have already revoked the certificate.  People can test if their computer is affected by the bogus certificate by following this link.  Ars Technica is reporting that at least two models of Dell laptop have been confirmed to contain the rogue certificate, but the actual number is possibly much higher. The same certificate appears to be installed in every affected Dell machine, which would enable an attacker to compromise every affected Dell user if only they had the private key which Dell used to create the certificate. Fortunately for attackers (and unfortunately for Dell's customers), Dell included that key on all the affected laptops as well. The result is that anyone with an affected Dell laptop could use it to create a valid HTTPS certificate for any other affected Dell laptop owner. One security researcher made this test site signed with the Dell certificate to prove that this attack was possible. During the test, the researcher confirmed that Firefox, Chrome and Internet Explorer all established an encrypted connection to the site with no warnings at all on an affected Dell laptop. Notably the Dell root certificate was also discovered on at least one SCADA system (the type of computer systems used to control industrial equipment, including in power plants, water treatment centers, and factories). Less than 24 hours after Ars Technica published the story, Dell issued an apology stating: Customer security and privacy is a top concern and priority for Dell; we deeply regret that this has happened and are taking steps to address it. The certificate is not malware or adware. Rather, it was intended to provide the system service tag to Dell online support allowing us to quickly identify the computer model, making it easier and faster to service our customers. This certificate is not being used to collect personal customer information. It’s also important to note that the certificate will not reinstall itself once it is properly removed using the recommended Dell process. Dell has also released an application to uninstall the certificate [exe] and instructions for how to remove the root certificate manually. While we applaud Dell for responding to this fiasco so quickly, the fact remains that it never should have happened in the first place. The rogue eDellRoot certificate is dated two months after the Superfish debacle happened. Furthermore, Dell used the Superfish debacle to their advantage, promoting the security of their own products. Since Dell clearly knew that installing a root certificate—à la Superfish—was a bad idea, why did they make the exact same blunder? We hope that other computer manufactures will learn from this fiasco, if they didn't already learn from Lenovo and Superfish. Hardware manufacturers need to realize that installing their own root certificates on consumer machines is dangerous and irresponsible, since it compromises the security of the entire web. If they don't they're guaranteed to keep facing embarrassment and losing the trust of their customers. Plenty of businesses rely on third-party payers: parents often pay for college; insurance companies pay most health care bills. Reaching out to potential third-party payers is hardly a new or revolutionary business practice. But someone should tell the Patent Office. Earlier this year, it issued US Patent No. 9,026,468 to Securus Technologies, a company that provides telephone services to prisoners. The patent covers a method of “proactively establishing a third-party payment account.” In other words, Securus patented the idea of finding someone to pay a bill. It’s been an interesting few weeks for Securus. First, the FCC announced that in response to price gouging by the industry, it would impose per-minute price caps on prison calls. Then The Intercept reported on a massive hack of recorded Securus calls: 70 million recordings, including many calls made under attorney-privilege, were leaked through SecureDrop. We’d like to add November’s Stupid Patent of the Month award. Securus’ patent has a single independent claim with three steps. These steps are: 1) identifying a “prospective third-party payer”; 2) detecting a “campaign triggering event” (this can be something like an inmate being booked into a facility); and 3) “initiating a campaign to proactively contact” the prospective third-party payer using an “interactive voice response system.” In other words, when an inmate gets booked into the local jail, Securus robocalls a family member to ask if they are willing to set up a pre-paid phone account. There are two serious problems with this patent. First, the claims are directed to a mind-numbingly mundane business practice and should have been rejected as obvious. Obvious uses or combinations of existing technology are not patentable. Second, the claims are ineligible for patent protection under the Supreme Court’s 2014 decision in Alice v. CLS Bank—this is a recent Supreme Court decision that holds that an abstract idea (like contacting potential third-party payers) doesn’t become eligible for a patent simply because it is implemented using generic technology. That the system failed to register either of these defects shows deep dysfunction. In a sane world, a patent examiner would apply common sense and reject Securus’ application out of hand. It includes no technological innovation (it notes that all of the relevant phone technology already exists); instead, it simply describes a basic set of steps for contacting potential third-party payers. Unfortunately, the Federal Circuit has essentially beaten common sense out of the patent system. For example, it recently overruled an examiner who had relied on common sense for the basic fact that electrical plugs have prongs. This repudiation of common sense is how we get patents on filming a yoga class or white background photography. To the patent examiner’s credit, he did originally reject all of Securus’ claims as obvious based on a combination of earlier publications regarding third-party payment accounts. Securus appealed that rejection to the Patent Trial and Appeal Board (PTAB). The PTAB overruled (PDF) the examiner. In a victory of hyper-formalism over rationality, the PTAB said that the examiner had not provided sufficiently explicit reasons for combining the teachings of prior publications. What about Alice v. CLS Bank? Securus’ claims are just an abstract idea implemented on generic technology. They should therefore have been found ineligible under the Supreme Court’s new standard. In fact, when it overruled the examiner on obviousness grounds, the PTAB explicitly noted (in a footnote) that the examiner “may wish to review the claims for compliance under 35 U.S.C. § 101” in light of the Alice decision. Yet the Patent Office ignored this suggestion and rubber stamped the claims. We have repeatedly urged the Patent Office (here, here, and here) to do a better job applying Alice to pending applications. It is very disappointing to see patents like this one being granted months after the Supreme Court’s ruling. Even invalid patents are very expensive to defeat in court after they’ve issued. Securus already has a captive market for its services. It does not need the monopoly power of a stupid patent as well. It is a truth universally acknowledged that a government, in the wake of a national security crisis—or hostage to the perceived threat of one—will pursue and in many cases enact legislation that is claimed to protect its citizens from danger, actual or otherwise. These security laws often include wide-ranging provisions that do anything but protect their citizens' rights or their safety. We have seen this happen time and time again, from the America's PATRIOT Act to Canada's C-51. The latest wave of statements by politicians after the Paris bombing implies we will see more of the same very soon. Not keen to be left out, Pakistan has now joined the ranks of countries using “cybercrime” and terrorism to rewrite the protections for their nationals' privacy and right to free expression. In January 2015 the Government of Pakistan drafted the Prevention of Electronic Crimes Bill (PECB). Ostensibly the PECB was written to address new digital issues, such as cyberstalking, forgery, and online harassment. The reality is the PECB contains such broad legal provisions that that it would criminalize everyday acts of expression while undermining the right to privacy of Pakistani citizens. PECB was introduced in the same period as the government of Pakistan established its National Action Plan, a comprehensive state-level project to combat terrorism after armed men linked to the Taliba, attacked an Army-run school in the city of Peshawar, killing 145 people, 132 of which were children. The PECB became part of the NAP: a political product intended to make control of political expression an official role of the government. Much like its international counterparts, the PECB skews in favor of national security—loosely defined—while ignoring civil liberties. Section 34 of the PECB, for example, gives the Pakistan Telecommunication Authority (PTA) powers to block objectionable content and websites, with very vague, unclear ideas as to what constitutes ‘objectionable'. If the PTA determine that it is “necessary in the interest of the glory of Islam or the integrity, security or defence of Pakistan or any part thereof, friendly relations with foreign states, public order, decency or morality,” then the authorities can censor it. Do you pass messages via Facebook, Twitter and other social media platforms? Under the PECB, if those messages are “obscene” or “immoral”, you may be committing a criminal offence—again, there is no clear definition of what constitutes either “obscene” or “immoral.” . Even if one does manage to think clean thoughts, sending an email or a message without the recipients permission is a criminal offence, under Section 21. A lack of clearly defined clarifications and explanations gives sweeping power to investigating agencies, with the ability to implicate, fine and imprison anyone for sending a single email without prior consent. These provisions and others in the drafted bill have led to condemnation from Pakistani rights organizations, international groups including Article 19, Human Rights Watch and Privacy International, and from Pakistan's legal and media communities. My own organization and many others have been pushing Pakistan's government to retract the drafted PECB, and to include amendments that incorporate civil liberties concerns. The political atmosphere has made them generally reluctant to open up the drafting process to civil society. Organizations, activists and members of Pakistan's nascent tech industry spent most of 2015 calling upon the Pakistan National Assembly's Standing Committee on Information Technology and Telecommunication to withdraw the drafted PECB for further study and amendments. On September 17th, however, the Standing Committee decided to approve the draft and send it on its way to the National Assembly. Actually, to be more precise: copies of the draft were not given by the drafters to other committee members. When they objected, and stressed that the drafted bill could not be approved without review, they were overruled by the committee chair, who said that as he had seen the draft, that would be sufficient to pass it onto the National Assembly. Anusha Rehman, the Minister of State for IT & Telecommunications, has defended the PECB, asserting that “safeguards have been ensured against any expected misuse.” But as it is currently written, the PECB contains little in the way of safeguards. Suggestions by civil society and lawyers have been consistently ignored. What Pakistan needs is a a cybercrime bill that progressively and effectively balances security and civil liberties. The current PECB text, badly drafted and politically compromised, is so far away from that goal that it needs a complete overhaul. Pakistan's lawmakers need to know how broken the PECB is. EFF and Digital Rights Foundation have created a tool that lets you send a message to key Senators and Members of the National Assembly via Twitter. Take action now, and stop the PECB from undermining Pakistan's online future. This guest post was written by Nighat Dad, the founder and executive director of Pakistan's Digital Rights Foundation, and research associate Adnan Chaudhry.  We wrote earlier this month about the Consumer Review Freedom Act (S. 2044, H.R. 2110), a bill that would prohibit businesses from using form contracts to prevent their customers from sharing negative reviews of their products and services online, or using bogus copyright claims to censor reviews they don’t like. We also joined a group of peer organizations in signing a letter in support of the bill. Last week, the Senate Commerce Committee approved an amended version of the bill, readying it for debate on the Senate floor. We applaud the committee for making customers’ freedom of speech a priority. Chairman John Thune (R-SD) is the bill’s primary sponsor in the Senate. Last time we wrote about the bill, we noted two changes we’d like to see to it before it passes. First, we were worried about a carveout that would allow a business to use a contract to assign the copyright for a customer’s speech to the business itself when that speech is not “lawful.” We think that this loophole could allow businesses to bypass the traditional protections in place for allegedly unlawful speech. For example, it’s easier and faster to send a copyright takedown notice to the offending user than to convince a judge to order the user to remove content for defamation. Unfortunately, the version the Commerce Committee approved still includes this loophole. Second, we were concerned that the law could be used to prosecute not only the businesses that offer these unfair contracts, but also the customers who enter into them. The committee did edit the definition of “form contract” to make it clear that the contracts the law addresses are those that establish a business-customer relationship, but we still fear that the law could be used against customers. The new version also clarifies that nonexclusive licenses to use content aren’t covered under the ban on contracts that transfer the customer’s copyright. That’s a good move: nearly all online forums and social media platforms require users to grant the platform a nonexclusive license to use their content. Calling those agreements into question clearly isn’t the intention of this bill. Once again, we’re glad that the Senate is moving forward with the CRFA. It’s not a perfect solution, but it’s great to see Congress addressing the problem of unfair, lopsided form contracts. No posts found We were out on the streets this week to march against the Trans-Pacific Partnership (TPP) agreement in the U.S. Capitol. We were there to demonstrate the beginning of a unified movement of diverse organizations calling on officials to review and reject the deal based on its substance, which we can finally read and dissect now that the final text is officially released.  Contained within these 6,000-plus pages of the completed TPP text are a series of provisions that empower multinational corporations and private interest groups at the expense of the public interest. Civil society groups represent diverse concerns, so while we may disagree on our specific concerns about the TPP, we commonly recognize that this is a toxic, undemocratic deal that must be stopped at all costs.  So on Monday, we kicked off the new phase of TPP campaigning to call on U.S. Congress members to reject the entire deal in the coming ratification vote in a few months.  Roughly a couple of hundred people came out to meet in front of the Chamber of Commerce. Some organizers and leading activists gave speeches about the impacts of the TPP on our local and global communities. Maira Sutton, EFF's Global Policy Analyst, spoke about the effects of the TPP's restrictive digital policy provisions that empower the rights of Hollywood and other corporations, and that it does little to nothing to safeguard the rights of the public interest on the Internet or over our digital devices. Other speakers discussed how the TPP would impact environmental protections and the raise the costs of affordable life-saving medicines and treatments. We then started the march, with large banners and people carrying dozens of toilet paper-shaped lanterns with the words "flush the TPP" written across it.  The rally picked up many more people as we snaked around the downtown area and marched towards the Ronald Reagan International Trade Center:  Another rally was held on Tuesday morning, where we marched to each of the TPP country embassies to demonstrate our support of those who have been protesting it in other regions of the world. Protesters carried a 10-foot-tall figure of Mr. Monopoly, which puppeteered the flags of the 12 TPP countries participating countries. Others carried flags with "stop TPP" in all the languages of the TPP countries, and a gigantic globe of the earth on their shoulders to signify our common responsibility for the rights and interests of people and environments worldwide:   People from all over the United States came to attend these events in DC this week. We met people from Texas, Alabama, Florida, North Carolina, Michigan, and Washington state. They all traveled hundreds or thousands of miles to voice their opposition against the TPP, as well as the other secretive trade deals that harm our digital rights and actively erode transparent, public-interest driven policymaking.  While we had a pretty good turn out of several hundred people at these events at the Capitol, a recent poll showed that 60% of people in the United States have no opinion on the TPP. Clearly, we still have a lot of work to do to make more people in the United States aware and actively working to stop this deal before it goes to Congress. Stay tuned as we develop more materials and resources to spread the word about the TPP's impacts on your digital rights. For now, you can start by taking this action to urge your lawmakers to call a hearing on the contents of the TPP that will impact your digital rights, and more importantly, to vote this deal down when it comes to them for ratification:     Europe is very close to the finishing line of an extraordinary project: the adoption of the new General Data Protection Regulation (GDPR), a single, comprehensive replacement for the 28 different laws that implement Europe's existing 1995 Data Protection Directive. More than any other instrument, the original Directive has created a high global standard for personal data protection, and led many other countries to follow Europe's approach. Over the years, Europe has grown ever more committed to the idea of data protection as a core value. In the Union's Charter of Fundamental Rights, legally binding on all the EU states since 2009, lists the “right to the protection of personal data” as a separate and equal right to privacy. The GDPR is intended to update and maintain that high standard of protection, while modernising and streamlining its enforcement. The battle over the details of the GDPR has so far mostly been a debate between advocates pushing to better defend data protection, against companies and other interests that find consumer privacy laws a hindrance to their business models. Most of the compromises between these two groups have now already been struck. But lost in that extended negotiation has been another aspect of public interest. By concentrating on privacy, pro- or con-, the GDPR as it stands has omitted sufficient safeguards to protect another fundamental right: the right to freedom of expression, “to hold opinions and to receive and impart information... regardless of frontiers”. It seems not to have been a deliberate omission. In their determination to protect the personal information of users online, the drafters of the GDPR introduced provisions that streamline the erasure of such information from online platforms—while neglecting to consider those who published that information to those platforms who were exercising their own human right of free expression in doing so, and their audiences who have the right to receive such information. Almost all digital rights advocates missed the implications, and corporate lobbyists didn't much care about the ramifications. The result is a ticking time-bomb that will be bad for online speech, and bad for the future reputation of the GDPR and data protection in general. Europe's data protection principles include a right of erasure, which has traditionally been about the right to delete data that a company holds on you, but has been extended over time to include a right to delete public statements that contain information about individuals that is “inadequate, irrelevant or excessive”. The first widely-noticed sign of how this might pose a problem for free speech online came from the 2014 judgment of the European Court of Justice, Google Spain v. Mario Costeja González—the so-called Right to Be Forgotten case. We expressed our concern at the time that this decision created a new and ambiguous responsibility upon search engines to censor the Web, extending even to truthful information that has been lawfully published. The current draft of the GDPR doubles down on Google Spain, and raises new problems. (The draft currently under negotiation is not publicly available, but July 2015 versions of the provisions that we refer to can be found in this comparative table of proposals and counter-proposals by the European institutions [PDF]. Article numbers referenced here, which will likely change in the final text, are to the proposal from the Council of the EU unless otherwise stated.) First, it requires an Internet intermediary (which is not limited to a search engine, though the exact scope of the obligation remains vague) to respond to a request by a person for the removal of their personal information by immediately restricting the content, without notice to the user who uploaded that content (Articles 4(3a), 17, 17a, and 19a.). Compare this with the DMCA takedown notices, which include a notification requirement, or even the current Right to Be Forgotten process, which give search engines some time to consider the legitimacy of the request. In the new GDPR regime, the default is to block. Then, after reviewing the (also vague) criteria that balance the privacy claim with other legitimate interests and public interest considerations such as freedom of expression (Articles 6.1(f), 17a(3) and 17.3(a)), and possibly consulting with the user who uploaded the content if doubt remains, the intermediary either permanently erases the content (which, for search engines, means removing their link to it), or reinstates it (Articles 17.1 and 17a(3)). If it does erase the information, it is not required to notify the uploading user of having done so, but is required to notify any downstream publishers or recipients of the same content (Articles 13 and 17.2), and must apparently also disclose any information that it has about the uploading user to the person who requested its removal (Articles 14a(g) and 15(1)(g)). Think about that for a moment. You place a comment on a website which mentions a few (truthful) facts about another person. Under the GDPR, that person can now demand the instant removal of your comment from the host of the website, while that host determines whether it might be okay to still publish it. If the host's decision goes against you (and you won't always be notified, so good luck spotting the pre-emptive deletion in time to plead your case to Google or Facebook or your ISP), your comment will be erased. If that comment was syndicated, by RSS or some other mechanism, your deleting host is now obliged to let anyone else know that they should also remove the content. Finally, according to the existing language, while the host is dissuaded from telling you about any of this procedure, they are compelled to hand over personal information about you to the original complainant. So this part of EU's data protection law would actually release personal information! What are the incentives for the intermediary to stand by the author and keep the material online? If the host fails to remove content that a data protection authority later determines it should have removed, it may become liable to astronomical penalties of €100 million or up to 5% of its global turnover, whichever is higher (European Parliament proposal for Article 79). That means there is enormous pressure on the intermediary to take information down if there is even a remote possibility that the information has indeed become “irrelevant”, and that countervailing public interest considerations do not apply. These procedures are deficient in many important respects, a few of which are mentioned here: More details of these problems, and more importantly some possible textual solutions, have been identified in a series of posts by Daphne Keller, Director of Intermediary Liability at the Center for Internet and Society (CIS) of Stanford Law School. However at this late stage of the negotiations over the GDPR in a process of “trialogue” between the European Union institutions, it will be quite a challenge to effect the necessary changes. Even so, it is not too late yet: proposed amendments to the GDPR are still being considered. We have written a joint letter with ARTICLE 19 to European policymakers, drawing their attention to the problem and explaining what needs to be done. We contend that the problems identified can be overcome by relatively simple amendments to the GDPR, which will help to secure European users' freedom of expression, without detracting from the strong protection that the regime affords to their personal data. Without fixing the problem, the current draft risks sullying the entire GDPR project. Just like the DMCA takedown process, these GDPR removals won't just be used for the limited purpose they were intended for. Instead, it will be abused to censor authors and invade the privacy of speakers. A GDPR without fixes will damage the reputation of data protection law as effectively as the DMCA permanently tarnished the intent and purpose of copyright law. The Peruvian digital rights organization, Hiperderecho, together with the Electronic Frontier Foundation, launched ¿Quién Defiende Tus Datos? (Who Defends Your Data?) today, a report that evaluates the privacy practices of digital communication companies that Peruvians use every day. Along with similar reports published earlier this year in Colombia and Mexico, this investigation is part of a larger series of evaluations across Latin America that is based on EFF’s annual Who Has Your Back? report and adapted for local realities and needs. The reports compare phone companies and Internet Service Providers to determine which ones stand by their users when responding to government requests for personal information. Peru is experiencing a digital revolution; its citizens are increasingly using the Internet and electronic devices to exercise free speech, organize social movements, and gather information. As more and more Peruvians use mobile phones and computers to access the Internet, more of their private data gets shared among companies who provide these services. As of July 2015, the government has been taking advantage of this shift, proposing brand new surveillance laws that compel ISPs to retain metadata for a certain period of time and allow warrantless access to geolocation data in emergency cases. As such, Hiperderecho has released the ¿Quién Defiende Tus Datos? report that evaluates whether Peruvian ISPs and telephone companies stand by their customers when the government knocks at their door compelling user data. From its inception, this project has had two main goals: to provide users with a clear assessment of which telecommunications companies are adopting best practices to protect their users’ privacy; and, to provide companies with guidance and recommendations on how they can improve their privacy practices. In their report, Hiperderecho analyzed whether companies publish appropriate and easy-to-understand privacy policies on their websites and if the outlined practices are sufficient enough to inform users about how they treat government requests.  Most of the companies have yet to earn a good evaluation in this first edition of the report, with some of them not even obtaining partial stars. As a result, telecommunications companies in Peru still have a long way to go to ensure the privacy of their users’ communications personal data. In categories like “Transparency Reports” and “User Notification Procedures,” no companies were awarded a star. In several cases, companies limited themselves to publishing privacy policies that neglected to include either what kind of data they were collecting or how long they would be storing the data. Peru’s  recent adoption of a new data protection law has forced companies to disclose their data collection practices every time they sign up new users, but the law doesn’t compel them to provide a more comprehensive evaluation of the data they collect as a by-product of the usage of the service. Hence, there is little information on how companies treat information they collect from users, like IP addresses, traffic logs, and geolocation, among others. This report asks companies to stand with their customers by implementing best practices to the fullest extent permitted by law. However, one of its key findings is that certain legal restrictions in Peruvian national law may prevent operators from adopting internationally-recognized best practices for user notice, which are designed to empower users to defend their own privacy. According to the Criminal Procedure Code or the rules of the national intelligence system, ISPs and mobile companies are compelled to keep government access requests confidential. Accordingly, the companies may be prevented from notifying their users upfront. However, there’s still much more that companies could do within the space of their legal obligations. Under Peruvian law, courts must notify citizens after a surveillance measure has expired and, when this happens, companies could contact them in parallel through email or text message to call their attention on the notification. This would allow citizens to exercise their right to oppose and appeal any surveillance measure previously issued by the courts. Some regional companies have better practices in countries other than Peru. For example, most of the Mexican companies, including Telmex (a subsidiary of América Móvil), have a privacy policy published on their website. However, Claro’s website in Peru does not publish this information. Peruvian companies still have a long way to go in protecting customers’ personal data and being transparent about who has access to it. Hiperderecho expects to release this report annually to incentivize companies to improve transparency and protect users data. By making privacy policies accessible and understandable, Peruvians will know how their personal data is used and how it is controlled by ISPs so they can make smarter consumer decisions. Check the full report of Who Defends Your Data? [Spanish] [PDF].  La organización peruana defensora de los derechos digitales Hiperderecho junto con la Electronic Frontier Foundation, lanzaron hoy “¿Quién Defiende Tus Datos?”, un reporte que evalúa las prácticas de privacidad de las empresas de comunicación digital que los peruanos utilizan cada día. Junto con reportes similares lanzados a mediados de este año en Colombia y México, estas investigaciones forman parte de una amplia serie de evaluaciones a lo largo de América Latina basadas en el reporte anual de EFF “Who Has Your Back?” y adaptado a las necesidades y realidades locales. Este reporte pone en la mira a las compañías de telefonía e internet para determinar cuáles se destacan ante sus usuarios al responder a las solicitudes de información personal por parte del gobierno. Perú está experimentando una revolución digital: Sus ciudadanos están utilizando cada vez más Internet y los dispositivos electrónicos para ejercer el derecho a la libertad de expresión, organizar movimientos sociales, y obtener información. A medida que más peruanos usan los teléfonos móviles y las computadoras para acceder Internet, cada vez más datos privados son compartidos entre las compañías que proveen estos servicios. Sin embargo, en julio de 2015 el gobierno ha aprovechado este cambio al proponer una ley que obliga a los Proveedores de Servicios de Internet (ISP en inglés) a retener metadatos de las comunicaciones por un periodo de tiempo y permitir el acceso sin orden judicial a datos de geolocalización en casos de emergencia. Bajo este contexto, Hiperderecho ha lanzado el reporte “¿Quién Defiende Tus Datos?” en el que evalúa si los ISP peruanos y compañías telefónicas protegen a sus usuarios cuando el gobierno golpea sus puertas exigiendo los datos personales de sus usuarios. Desde sus inicios, este proyecto ha tenido dos objetivos principales: proporcionar a los usuarios con una evaluación clara de qué empresas de telecomunicaciones están adoptando las mejores prácticas para proteger la privacidad; y proporcionar a las empresas con una guía y recomendaciones sobre cómo pueden mejorar sus prácticas de privacidad. En este informe, Hiperderecho analizó si las compañías publican políticas de privacidad adecuadas y fáciles de entender en sus sitios web oficiales y si las prácticas descritas son suficientes para informar a los usuarios acerca de cómo se tratan los pedidos del gobierno. Son cinco criterios de evaluación y el puntaje otorgado es visible mediante una estrella completa, media estrella o un cuarto de estrella.  La mayoría de las compañías deberían obtener una buena evaluación incluso en esta primera edición del informe, aunque algunas ni siquiera obtuvieron media estrella. Como resultado, las compañías de telecomunicaciones en el Perú tienen un largo camino que recorrer para proteger la privacidad de los datos de comunicaciones de sus usuarios. En categorías como "Informes de Transparencia" y "Notificación al usuario," no existen compañías que se ganaran una estrella. En varios casos, las empresas se limitaron a publicar las políticas de privacidad en las que no incluyeron el tipo de datos que recopilaban o por cuánto tiempo se almacenan los datos. La reciente aprobación en el Perú de una nueva ley de protección de datos ha obligado a las compañías a divulgar sus prácticas de recolección de datos cada vez que se sumen nuevos usuarios, pero la ley no los obliga a proporcionar una evaluación más completa de los datos que recogen como un subproducto de la utilización del servicio. Por lo tanto, hay poco conocimiento sobre la forma en que tratan la información que recopilan de los usuarios tales como direcciones IP, registros de tráfico y de geolocalización, entre otros. En este informe se urge a las compañías a estar del lado de sus clientes mediante la implementación de más y mejores prácticas en la medida permitida por la ley. Sin embargo, una de los principales hallazgos es que ciertas restricciones legales en la legislación nacional peruana pueden evitar que los operadores de telecomunicaciones adopten las mejores prácticas internacionalmente reconocidas para la notificación a usuarios, diseñadas para facultar a los usuarios la posibilidad de defender su propia privacidad. De acuerdo con el Código de Procedimiento Penal o las reglas del sistema nacional de inteligencia, las compañías de telefonía móvil e internet están obligadas a mantener confidencialidad sobre los pedidos de acceso del gobierno. En consecuencia, las compañías pueden estar impedidas de notificar a sus usuarios por adelantado. Sin embargo, todavía hay mucho más que estas compañías podrían hacer dentro de sus obligaciones legales. Bajo la ley peruana, los tribunales deben notificar a los ciudadanos después de que una medida de vigilancia haya expirado y, cuando esto sucede, las compañías podrían contactar en paralelo a los usuarios mediante correo electrónico o teléfono para informarles sobre la notificación. Esto permitiría que los ciudadanos ejerzan su derecho a oponerse y apelar cualquier medida de vigilancia expedida previamente por los tribunales. El informe revela que algunas compañías de alcance regional tienen mejores prácticas en países distintos a Perú. Por ejemplo, la mayoría de las empresas mexicanas, incluyendo Telmex (subsidiaria de América Móvil), tienen una política de privacidad publicada en su sitio web. Sin embargo, el sitio web de Claro en Perú no publica esta información. Las compañías peruanas aún tienen un largo camino por recorrer en pos de proteger los datos personales de los clientes y ser transparentes acerca de quién tiene acceso a ella. Hiperderecho aguarda publicar este informe anualmente para incentivar a las empresas a mejorar la transparencia y la protección de los datos de usuarios. Al hacer políticas de privacidad accesibles y comprensibles, los peruanos sabrán cómo se utiliza su información personal y cómo es controlada por las compañías de telefonía e internet para que puedan tomar decisiones de consumo más inteligentes. La versión completa del reporte ¿Quién defiende tus datos?: Reporte de evaluación de empresas de telecomunicaciones ante las medidas de vigilancia estatal puede ser descargada en PDF desde aquí.  In what we very much hope launches a “race to the top” to protect online fair use, today YouTube announced a new program to help users fight back against outrageous copyright threats. The company has created a ‘Fair Use Protection’ program that will cover legal costs of users who, in the company’s view, have been unfairly targeted for takedown. We have criticized YouTube in the past for not doing enough to protect fair use on its service, including silencing videos based on vague “contractual obligations” and failing to fix the many problems with its Content ID program. However, when the company takes positive steps to protect its users, we take notice. Google describes the program on its blog, but here are the basic details: When the company notices that a video targeted for takedown is clearly a lawful fair use, it may choose to offer the user the option of enrolling their video into the program. If the user decides to join, the video will stay up in the United States and, if the rightsholder sues, YouTube will provide assistance of up to $1 million dollars in legal fees. YouTube has started the program off with four videos that the company believes represent fair use. You can watch them here. While we would like the program to do a little bit more—for example, given that the main criteria is that a video must be clearly lawful we’d like YouTube to provide any user that meet that criteria the option of enrolling their video into the program, rather than hand-selecting which ones gets to participate—we think this is a solid and unprecedented step forward in protecting fair use on the site. We commend YouTube for standing up for its users, and we hope the program will inspire other service providers on the web to follow its lead. Like clockwork, cynical calls to expand mass surveillance practices—by continuing the domestic telephone records collection and restricting access to strong encryption—came immediately following the Paris attacks. These calls came before the smoke had even cleared, much less before a serious investigation completed. They came from high places too, including CIA head John Brennan and New York Police Commissioner Bill Bratton. Seasoned law enforcement officers and the heads of spy agencies should know better than jump to conclusions before the facts are in. Sadly, these premature demands for more surveillance in the wake of tragedies are not unprecedented. The most prominent example is the Bush Administration's aggressive push for expanded surveillance powers after the 9/11 terrorist attacks, before a proper investigation could be carried out. We all now know from the 9/11 Commission that the Bush Administration failed to uncover the attacks and stop them not because of insufficient legal authority and not because they didn't have sufficient information, but because of operational failures and internal rules. Yet despite this, the Bush Administration rushed to Congress to give it broad new collection authorities in the USA Patriot Act. We also now know that along with the public law, the government used secret legal interpretations to gather even more data about innocent people, interpretations that have since been revealed as both shocking and unsupported. In response to its failure to properly act on the data it had, the government pushed to collect even more data.  Now we see a sadly similar exploitation of this latest international tragedy, again pushed by people who are supposed to be above petty politics. First, Sen. Tom Cotton has floated a bill suggesting that, as a result of the Paris tragedy, we continuing throwing money at the domestic telephone record collection program—which was itself based on an improper interpretation of section 215 of the Patriot Act. The program is set to end on November 29, 2015, switching from mass surveillance to a model of surveillance that is still too broad, but more targeted than the indiscriminate dragnet of the existing system. One major reason Congress ended the broader program is that it didn’t work. Millions of dollars and over 10 years of effort later, two independent panels held there was no indication that the mass domestic telephone collection had ever assisted in thwarting a domestic terrorist attack. Of course the 215 program hasn’t even ended yet, so if it could have been useful in stopping the Paris attack—an unlikely proposition, given its domestic focus—it failed at that too. More relevant, the massive collection program the government continues in the U.S. purportedly aimed at foreigners abroad under FISA Amendments Act section 702 failed to catch the Paris terrorists before they struck, as did the even bigger set of collections occurring abroad under Executive Order 12333, which include collections aimed at both France and Belgium, where the terrorists were allegedly based. That’s because big data and mass surveillance techniques are simply not useful for predicting or uncovering terrorist plots. Terrorism is far more difficult to predict than the purchasing or other patterned behavior that big data is reasonably good at identifying. Forget trading essential liberty for a little temporary safety: when it comes to identifying terrorists, mass surveillance leaves the public with neither. So whether the focus is on spending our money on things that actually work or on protecting our Constitutional rights and our ability to be "secure" in our papers in the digital age, it’s long past time we shifted focus from the expensive “collect it all” strategies to more focused surveillance. Second, the attack on strong, non-backdoored encryption would make Americans, and people all over the world, less secure. Every serious computer scientist has pointed out that there is no such thing as a back door that only good guys can go through. And at least so far, the information we’ve received is that end-to-end encryption wasn’t even used here. The world is rightly horrified at what happened in Paris. We understand the desire to do “something” to shore up our security. But terrorism is aimed, in part, at pushing us to jump to conclusions and take panicky steps that inflict more pain and misdirect our resources toward failed and dangerous ideas. Luckily this time many voices are urging caution and careful analysis, and rejecting the cynical ploy of some to use our terror to take expensive and dangerous steps in the wrong direction. The U.S. Trade Representative (USTR) fears the grassroots tech community, and rightly so. Internet users are the community that killed SOPA and PIPA in the U.S. Congress and ACTA in the European Parliament. The USTR is right to fear that the same could happen to the Trans-Pacific Partnership agreement (TPP). That's why they've taken such pains to present the TPP as being friendly to the Internet and tech users and have included a few provisions in the agreement that they can point to to justify this claim. We've covered (and debunked) some of these before—notably the free flow of information rules common to both TISA and the TPP—but there's another that deserves comment. Under the heading “How the TPP Protects the Internet and Ensures Digital Freedom,” the USTR claims on its website that the TPP “ensures that companies and individuals are able to use the cybersecurity and encryption tools they see fit, without arbitrary restrictions that could stifle free expression.” This refers to a heretofore obscure provision hidden away in Annex 8-B of the Technical Barriers to Trade [PDF] chapter of the TPP, which provides: With respect to a product that uses cryptography and is designed for commercial applications, no Party may impose or maintain a technical regulation or conformity assessment procedure that requires a manufacturer or supplier of the product, as a condition of the manufacture, sale, distribution, import or use of the product, to: The USTR's characterization of these provision certainly seems to have convinced former Homeland Security policy secretary and NSA lawyer, Stewart Baker, who went so far as to proclaim in the Washington Post that the USTR wins the Crypto War. In his interpretation, the provision would prevent a TPP country from requiring a supplier of cryptographic software to provide it with a backdoor or “golden key," of the kind that law enforcement authorities have been demanding and that we have consistently and strongly denounced. But this is much too rosy an interpretation, for several reasons. Most importantly, the provision quoted above is immediately followed by an exception whereby a service provider that uses encryption can still be required to provide unencrypted communications to law enforcement agencies pursuant to “legal procedures.” Since this is really all that law enforcement authorities are after, the fact that a provider can't actually be forced to disgorge the actual private key they are using, hardly matters at all. But, for the sake of argument, supposing the government does want a product's private key, rather than just the decrypted communications, the TPP still allows them a way to get it. The Technical Barriers to Trade chapter is only about standards with which products must comply in order to be approved for commerce. Thus it prohibits the requirement that a private key allowing decryption be handed over as a condition of manufacture, sale distribution, import or use of the product. But it wouldn't do anything to prevent the government from seeking a court order against a software vendor requiring it to disclose the private key of a product that is lawfully marketed or supplied within the country. Further, the Exceptions and General Provisions chapter provides that “Nothing in this Agreement shall be construed to … preclude a Party from applying measures that it considers necessary for the fulfilment of its obligations with respect to the maintenance or restoration of international peace or security, or the protection of its own essential security interests.” This lays the foundation for a government to override Annex 8-B altogether if it can claim that it considers it necessary to do so for national security reasons. As if the above loopholes weren't large enough already, consider that the provision in Annex 8-B is only enforceable by other TPP countries. This means that if, say, the United States government compelled a home-grown encryption product such as Wickr to embed an encryption backdoor, there would be no restriction of trade between the TPP countries and thus no actionable claim under the TPP. A similar situation would exist for products from non-TPP countries; nothing would prevent a TPP country from requiring the developers of, for instance, Telegram which is based in Germany, to backdoor their software. A claim under the TPP would only arise if the country demanding backdoor access to an encryption product, and the country from which that product is developed or supplied, are both different TPP signatories. So what appears on the surface to be strong protection for crypto software in the TPP is actually much weaker than it seems: it doesn't prevent the government from requiring providers to give them access to decrypted data, it doesn't protect developers against backdoor demands from their own government, it doesn't protect tools from countries that aren't TPP signatories, it doesn't stop a country from demanding access to private keys of a product so long as this demand is not a condition of supply of that product within the country, and on top of all that, there is a sweeping national security exception that can override the provision altogether. So much for winning the crypto wars. EFF and Public Knowledge filed comments today at the United States Patent and Trademark Office discussing proposed changes to Patent Office trials. Our comments focus on making the process more fair and accessible for small entities that need to challenge bad patents. Our first set of comments relates to proposed changes to inter partes review and covered business method review. Congress created these procedures in the America Invents Act, passed in 2011, to allow quicker and more efficient review of issued patents. We make a number of suggestion for how the current rules could be improved to promote fairness. For example, we argue that the rules should clearly require both petitioners and patent owners to support affirmative factual statements with evidence. Our comments also address an issue that arose during the Patent Office’s nationwide “Roadshow.” This Roadshow was a joint presentation of the Patent Office and the American Intellectual Property Law Association (the “AIPLA”). The Patent Office touted [PDF] the Roadshow as allowing the public to “provide valuable input into how to improve the fairness and effectiveness of the AIA proceedings.” Unfortunately, the public had to pay a steep attendance fee of $375 to attend this Roadshow (the fee was significantly less if the participant was a member of the AIPLA). In addition, even though the Roadshow included an actual patent trial, there was no indication that the public could attend the trial for free (as at least one court has said the Constitution requires). We were extremely disappointed in how the Patent Office carried out its Roadshow, both by affiliating with an organization with particular (sometimes controversial) views on patent policy and putting the price out of reach of many members of the public. In our comments, we highlight how problematic this was and our hope that the Patent Office will reconsider how it implements Roadshows in the future. Our second set of comments relate to the Patent Office’s proposal to run a pilot program where trials are initially reviewed by only one judge (instead of a three-judge panel). We have reservations about this proposal. As our comments explain in more detail, there are many benefits of multi-member, collegial, bodies. Some critics of the current process (such as the AIPLA) argue that having the same panel of administrative law judges making preliminary and final decisions in a proceeding creates bias against the patent owner. This is nonsense. The PTAB’s process is no different from federal courts where the same judge evaluates motions to dismiss, summary judgment, and then presides over trial. It would be extraordinarily wasteful for courts to bring in a different judge for every stage of a proceeding. The Patent Office should reject calls to create such a silly process for proceedings before the PTAB. Finally, last month we submitted comments regarding the Patent Office’s implementation of guidelines relating to the Supreme Court’s Alice decision on abstract software patents. This is the third time we’ve filed comments on Alice (we filed our earlier comments in March 2015 and August 2014). We remain very concerned that the Patent Office is not applying Alice diligently and ineligible patent claims are still being issued. We highlight some recently issued patents that we believe are invalid under Alice. We also urge the Patent Office to provide clearer guidance on how the law has changed. Once they issue, bad patents are the favorite tool of the patent troll and are extremely expensive to invalidate. We need the Patent Office to do a better job in the first instance so that real innovators aren’t left fighting abusive patent suits. No posts found On Friday, the major US movie studios quietly backed away from the worst parts of the censorship power-grab they attempted in July in the Paramount v. John Does (MovieTube) case. The studios are still hoping to take MovieTube’s Internet domain names away, but they are no longer asking for an order commanding the entire Internet to act as censors for them—a dangerous proposition that would open the door to more censorship and impede legitimate speech. The studios, members of the Motion Picture Association of America, sued the anonymous operators of the movie-streaming site MovieTube back in July, accusing them of copyright and trademark infringement. At that time, they asked the court for an immediate order, known as a preliminary injunction. The studios wanted an order that would apply to all “Internet service providers, back-end service providers, sponsored search engine or ad-word providers, merchant account providers, payment processors, shippers, domain name registrar[s] and domain name registries” — in short, the entire Internet. With that order in hand, the studios could force any intermediary, or all of them, into helping make the MovieTube site disappear.  We pointed out, at the time, why an order like the one the studios were asking for was extremely dangerous. The issue is not whether the MovieTube sites were infringing copyright or harming the movie studios, but rather that expanding the legal remedies for infringement will lead to other serious harms. Blocking entire websites almost always censors First Amendment-protected speech, and the power to block entire websites is a small step away from the power to dictate their contents. Conscripting Internet intermediaries to create site-blocking mechanisms makes the Internet less reliable and secure and emboldens other would-be censors, like repressive governments. This is the very power that the Internet blacklist bills SOPA and PIPA would have created, had they passed. In a rare moment of solidarity, Facebook, Google, Tumblr, Twitter, and Yahoo! filed a brief together in the MovieTube case to explain these dangers. They also pointed out that copyright and trademark law protect most Internet platforms against being forced to police or filter content posted by others—vital protections that the studios were trying to bypass. After the Internet companies filed their brief, the studios seemed to recognize that they had overreached. They dropped their request for a preliminary injunction and waited. The MovieTube defendants never appeared to defend themselves, so last Friday, the court declared a default, clearing the way for a final resolution of the case. The movie studios then asked the court for a permanent injunction. Surprisingly, the site-blocking powers they are asking for are narrower than their earlier ask. Gone are the references to ISPs, hosting providers, payment networks, ad networks, and search engines. The order now seems to cover only the defendants themselves, their close confederates, and domain name registrars and registries, who would be forced to turn the MovieTube domains over to the studios. It’s good to see the studios back away from the worst parts of their earlier grab for site-blocking power, although their new proposal still gives us cause for concern. The studios want an order that bans “index[ing] . . .link[ing] to . . . or otherwise us[ing]” their movies “or portion(s) thereof”. In other words, they want an order that makes non-infringing uses of movies, including fair uses, illegal. And they want that order to apply not only to the MovieTube defendants but to “any persons in concert or participation with them.” That phrase is important. Federal rules let a judge issue orders to bind people who are in “active concert or participation” with the defendant in a case, meaning a confederate or co-conspirator. Recently, some trademark and copyright owners, particularly major entertainment distributors, have tried to broaden the meaning of that phrase to include neutral service providers who handle all kinds of user data. By quietly dropping the key word “active” from their injunction proposal, the studios might still be trying to give themselves broad power to edit the Internet. Of course, major entertainment companies haven't ended their quest for site-blocking power. They continue to pursue it in court, through federal and state agencies, and by pressuring the companies that run the Internet's domain name system. We’ll be watching for signs of a broader power-grab by the studios in the MovieTube case, and we hope that service providers large and small stand up for their users by refusing to follow site-blocking orders that don’t properly apply to them.  Confirmed cases of misuse of California’s sprawling unified law enforcement information network have doubled over the last five years, according to records obtained by EFF under the California Public Records Act. That adds up to a total 389 cases between 2010 and 2014 in which an investigation concluded that a user—often a peace officer—broke the rules for accessing the California Law Enforcement Telecommunications System (CLETS), such as searching criminal records to vet potential dates or spy on former spouses. More than 20 incidents have resulted in criminal charges.  Unfortunately, those figures only represent what was self-reported by government agencies to the California Attorney General. The actual number of misuse cases of CLETS are likely substantially higher since the California Attorney General’s Department of Justice (CADOJ) has let many agencies slide on their annual misuse disclosures. Among the delinquent are two of California’s largest law enforcement agencies: the Los Angeles Police Department (LAPD) and the Los Angeles County Sheriff’s Department. What’s worse is the government body charged with overseeing disciplinary matters—the CLETS Advisory Committee (CAC)—seems to have taken no action to address the problem or ensure accountability from individual agencies.  Law enforcement abuse of confidential databases have been a growing concern for privacy and civil liberties groups like EFF. It occurs at all levels of government. In 2013, the NSA acknowledged that agents used intelligence systems to snoop on romantic interests (a practice dubbed “LOVEINT”). Last month, a Border Patrol supervisor was arrested and charged for allegedly manipulating a Homeland Security database to retaliate against a man who had made “child-rape” allegations against the supervisor’s brother.  Of the hundreds of cases of verified misuse of CLETS each year, only a handful of stories have reached the public, often years after the fact. Here are a few of the worst ways that police have abused the system in recent years: EFF began investigating CLETS after reviewing official “misuse statistics” presented in public hearings that made little sense and did not seem to reflect misuse at all. Digging deeper, we learned the CLETS Advisory Committee has aggressively moved to expand the system’s capabilities, while more often than not turning a blind eye to the also-growing misuse. Think of CLETS as California’s law enforcement “cloud.”  CLETS links together more than 5,200 unique “points of presence,” such as dedicated office computers and mobile terminals in patrol cars. It’s a system so large that CADOJ told EFF it doesn’t even keep a master list of which agencies have signed agreements to access the system. In addition, many CLETS features are accessible through a web app called “SmartJustice.” The system also allows CLETS users to send millions of messages to each other every day, such as all-points-bulletins and Amber alerts. CLETS users are granted access to whole universes of databases that don’t just contain information on Californians, but records from other states and the federal government. If you’ve got a California-issued ID, registered a car in California, received a parking citation, have any kind of criminal history or protective order, or any kind of record in 11 other databases, then you likely have files that can be accessed from CLETS. But that’s not all: CLETS also connects to Oregon’s equivalent network, which means if you’re an Oregonian, California police may be able to access your information too, especially if you drive a car. But those datasets pale in comparison to the access CLETS provides to an interstate database called NLETS and the FBI’s National Crime Information Center. A 15-part series of old-school CLETS training videos, chock-full of reenactments and animation, are available through Lemoore Police Department's Vimeo page. Under state law, there are two government bodies in charge of overseeing CLETS. The legislature assigned the California Attorney General the responsibility of administering CLETS on behalf of the state’s law enforcement agencies. But lawmakers also decided that the attorney general would take direction on policy and disciplinary matters from CAC (again, that stands for the CLETS Advisory Committee), a nine-member body that meets several times a year. Currently, members representing law enforcement and local government lobby groups have a voting majority. There are no members representing civil liberties or privacy organizations. Agencies that sign up for CLETS agree to follow the CLETS "Policies, Procedures and Practices"—essentially the system's terms of use. According to this rulebook, when a law enforcement agency investigates a CLETS violation, it is supposed to report what happened and what action was taken to the attorney general, which in turn is supposed to present the information to CAC. At that point, CAC is supposed to recommend a course of action for the CADOJ, which could include issuing a letter of censure, temporarily suspending the agency's access to CLETS, or discontinuing access altogether. CAC can also call the head of the agency (say, the chief of police) before the committee to explain what happened.  Over the last five years, CAC has never once pursued any of those measures against an agency over misuse of the system. In fact, there is nothing in CAC's meeting minutes to indicate that the body has ever publicly discussed the growing cases of misuse. Based on our research and discussions with CADOJ, it seems the agency is not enforcing reporting requirements, nor is it presenting what information it does collect to CAC. Meanwhile, CAC doesn’t seem to mind that it’s not being provided this information. The problem is circular: CADOJ can’t take action against misuse unless it has been directed to do so by CAC. And CAC can’t recommend an action against misuse unless CADOJ provides the committee with misuse reports. As a result, neither body seems to be addressing the issue.  (EFF could only identify one instance where CAC even discussed a particular misuse case, although it wasn't characterized as misuse at the time. In 2014, a Madera County Grand Jury investigation concluded that misuse of the CLETS terminal at the county jail resulted in the accidental release of an arrestee, who later killed a bystander during a car chase. According to CAC meeting minutes [.pdf], CADOJ only told the committee that Madera County was "not compliant with security awareness training" and would be given six months to get it together.) The CLETS agreement also requires each agency to file an annual report of misuse statistics. The information in these reports includes: number of misuse complaints the agency received, whether those complaints were received from internal or external sources, the outcome of the investigation, and what actions were taken. If criminal charges were filed, the agency must report if prosecution resulted in a conviction. CADOJ has not passed these statistics onto the oversight committee either. Instead, at each meeting, CADOJ staff present the committee with a series of numbers that they call “Misuse Statistics,” but are really nothing of the sort. Here’s an example of a slide presented by CADOJ at the March CAC meeting:  CAC generally glosses over this information during its meetings without asking questions. However, when EFF asked what these numbers actually mean, CADOJ staff explained that these numbers only show how many times the access log was checked for misuse. It does not, in any way, indicate actual misuse.  So, EFF filed a request under the California Public Records Act to get the real numbers for CLETS misuse.  The data was astounding: CLETS abuse more than doubled between 2010 and 2014. Agencies received 641 complaints over that period and between 586 and 619 investigations were conducted (the data is internally inconsistent). Approximately two-thirds of those investigations resulted in an affirmative finding that misuse had indeed occurred. Of those 389 cases of confirmed misuse, 109 resulted in no action taken at all. As for the rest:  Even these numbers fail to paint a complete picture of the problem. Currently, 143 misuse investigations remain mysteries; their outcomes are listed as simply “pending,” and the documents were never updated after the investigations were concluded. Of the 21 cases where users faced criminal charges, only four so far have resulted in convictions, with the dispositions of the remaining cases undisclosed.  In addition, even when an agency says it recorded zero CLETS misuse, that doesn't necessarily mean there was none. For example, Madera County didn’t report the 2013 jail case in its statistics because it didn’t start an investigation until a year after the incident (after the grand jury slammed the sheriff for failing to conduct an investigation). Furthermore, there are places where the numbers provided by agencies don't seem to add up.  More alarmingly, however, was our discovery that many agencies hadn’t filed disclosures at all. Because CADOJ doesn’t keep a master list of agencies that should be reporting, EFF had no way to determine how many agencies were delinquent. We were, however, able to confirm the Los Angeles County Sheriff’s Department did not file any disclosures between 2010 and 2014. LAPD—which caught an officer digging up information on murder witnesses in 2010—only filed a form once, in 2012. Calls to the sheriff department went unreturned, while LAPD staff could not determine who was responsible for filing the forms. Meanwhile, there’s nothing to indicate either the CADOJ or CAC ever followed up. There's one further reason to be wary of the data: the source material no longer exists.  In our initial request, we asked for each individual annual misuse report. Instead CADOJ provided us a series of tables, explaining that "once received, the data is entered onto a spreadsheet and the form destroyed." Throwing out the original records makes it difficult, if not impossible, to double-check inconsistencies in the data.  Download CLETS misuse data for the years 2010 through 2014 [.zip].  CAC does provide critical oversight in one capacity: ensuring agencies are in compliance with CLETS and FBI security standards—such as encryption, password strength, and training. For example, a March 2014 audit by the FBI found widespread compliance issues among 10 agencies, including failure to conduct appropriate training, failure to fingerprint all personnel with access to the system, and failure using sophisticated encryption. Many of these issues remained unresolved more than a year after they had been identified. Despite the skyrocketing misuse and the ongoing cybersecurity challenges, CAC has spent the last year coming up with new ways to expand CLETS. In December 2014, for example, CAC authorized CADOJ to link CLETS to an interstate driver license photo-sharing system, granting California police access to DMV photos from across the country. At CAC’s July 2015 meeting, the body quietly approved the 2015 Strategic Plan, which calls for expanding biometric data capture and sharing real time and historical GPS data on offenders statewide. EFF would like to see the California Attorney General and CAC do their jobs by properly monitoring CLETS and holding agencies responsible for misuse. CADOJ should collect the misuse information it is supposed to, stop destroying the original records, and provide that data to the official oversight committee. CAC, in turn, should openly discuss how CLETS policies can be improved to reduce the potential for abuse and recommend action against agencies that fail to comply. Sadly, these bodies have demonstrated they see little value in enforcing the rules and even less value in public participation. All year, EFF has been trying to ensure accountability with CLETS—filing public records requests, sending letters, and addressing the committee during public comment. Our goal so far has been to fight CLETS expansion plans and to demand greater transparency in how it conducts its meetings.  In March 2015, EFF demanded CAC drop its plans to integrate facial recognition technology with the California DMV photo database and share DMV photos with other states. After 1,500 supporters sent emails to CAC, the committee removed that goal from its strategic plan.  We were joined by the ACLU of California, Californians Aware, and First Amendment Coalition in a letter warning the committee that the way it is conducting its hearings is likely in violation of the state’s open meetings laws. At its July 2015 meeting, CAC responded that “convenience” for its members trumped the public’s right to meaningfully access and participate in decisions regarding CLETS. Then CAC voted to pass a 2015 Strategic Plan—a document that had never been publicly released or announced on an agenda before being finalized.  It may be time for the California legislature to step in to protect the privacy of their constituents. Measures could include holding investigative hearings, adding new, non-law enforcement members to the committees, and requiring full and public disclosures of misuse statistics.  In the meantime, you can count on EFF to remain vigilant. Stay tuned, because we may need your help. If you live in San Francisco (or spend much time on social media) you probably saw a lot of discussion last month about Proposition F, a controversial proposal to regulate short-term property rental services like Airbnb. You may also know that Airbnb spent millions opposing the measure, many times the budget of the proposition’s supporters. Here’s what you might not know: the bill’s opposition also got a little unexpected assistance from the DMCA (Digital Millennium Copyright Act) takedown process. Just a week before the vote, the only television ad supporting the measure disappeared from TV, YouTube, and the website of the organization that created it. Watch the ad yourself and see if you can guess why:
Did you catch it? The “Hotel San Francisco” lyric and the soundalike background music were enough to earn a cease-and-desist letter and DMCA takedown notice from attorneys representing The Eagles’ Don Henley and Glenn Frey. Henley and Frey’s lawyers threatened to sue for massive damages if ShareBetterSF didn’t pull the ad. If this story sounds familiar, that might be because political ads are often the targets of unfair DMCA takedowns. In 2008, the Obama and McCain campaigns were both hit with takedown notices for using news footage in their advertisements (from CBS and NBC, respectively). In 2009, NPR filed a YouTube takedown notice on an ad that criticized same-sex marriage; the ad had used a brief soundbite from an NPR program. In all three cases, a court would have recognized that the campaigns were within their rights. The ads use the clips simply to provide information; they don’t imply the news organizations’ endorsement or affect their viewership in any way. But thanks to the DMCA’s takedown-first-and-ask-questions-later procedure, none of them ever went to court. The makers of “Hotel San Francisco” were clearly in the right too, but it didn’t matter. As they told the Internet Archive, there was no purpose in challenging the takedown. Even if they had immediately counter-noticed, the ad still wouldn’t have been restored for 2 weeks—too late to have any effect on the election. Once again, the DMCA was used to shut down political expression with no consequences for the sender. As the 2016 campaigns get into full swing, expect to see more of this kind of abuse. This story might give you déjà vu for another reason: it’s not the first time Don Henley has accused the creators of a satirical political ad of copyright infringement. In 2010, Henley sued Senate candidate Charles DeVore for his parodies of two songs in political advertisements. A U.S. District Court rejected Henley’s claim that the ads falsely implied that he’d endorsed DeVore, but ruled that they did constitute infringement of his copyright. We disagreed with that ruling, but whatever you might think of it, the facts here are very different. ShareBetterSF selected “Hotel California” to make a specific political point. The use was noncommercial and highly transformative, and it couldn’t possibly harm any market for the original work—all factors favoring fair use. But none of that mattered in light of the takedown. Again and again, overzealous DMCA takedowns disregard fair use. That’s unfortunate, because fair use is designed to ensure that copyright law is compatible with the First Amendment. When fair use is overlooked in the face of a takedown notice, it really means that freedom of speech is compromised. EFF is fighting to make sure the targets of DMCA abuse can hold the abusers accountable. In the meantime, we have been documenting the worst abuses in our Takedown Hall of Shame. Given its dangerous consequences for political speech, this takedown has earned a spot.  The U.S. Department of Education (ED) is considering a rule change that would make the educational resources the Department funds a lot more accessible to educators and students—not just in the U.S., but around the world. We hope to see it adopted, and that it sets the standard for similar policies at other government agencies. Sign the petition The policy would require that grantees share all content that the ED funds under an open license. If the ED funds your work, you must share it under a license that allows anyone to use, edit, and redistribute it. For software, the license would also have to allow people to access and modify the source code. This is a great move. The Executive Branch funds billions of dollars’ worth of content through various departmental grant programs. Those grants are intended to benefit the public, but too often the public never even sees the results, let alone uses them. All federal grants are subject to a rule that allows the government itself to share grant-funded works with the public, but that rule has not been sufficient to make sure that sharing happens in practice. As the ED notes, the rule requires the public to be aware of the materials and contact the ED for access, both significant practical and informational hurdles. Open licenses allow publishers and other intermediaries to distribute materials much more widely than either the ED or grantees can alone. The existing policy also doesn’t cover reuse. Simply being able to access a resource isn’t enough. To unlock its real value, you need to be allowed to modify it, merge it with other resources, and republish it. That’s especially true in education: when educators are empowered to customize materials for their needs and share them with other educators, everyone benefits. Sometimes open access or open education policies authorize specific types of reuse, but those carveouts end up creating doubt and confusion about how people are allowed to use the content. Open licenses are the right solution. Sign the petition The EFF will be submitting a comment in support of the rule change. If you’d like to show your support too, then sign our petition. We’ll include your name with our comment. Update (November 30): The Department of Education has extended its deadline for comments to December 18. Please sign our petition by Wednesday, December 16. In general, Facebook has some pretty decent copyright policies. If you upload content to Facebook and it’s removed because of a bogus takedown request, you can file a counter-notice via a form on Facebook’s website. If the claimant doesn’t take action against you in a federal court in 14 days, your content is restored. That’s how it’s supposed to work, and Facebook usually does it right. Unlike some platforms, it also doesn’t ding users as “repeat offenders” based on multiple phony claims. But Facebook has recently introduced a new system for automatically recognizing copyright infringement in videos, and the way it works could raise a few eyebrows. In some circumstances, the new copyright bot actually requires Facebook users to share their private videos with a third party. While arguably well-intentioned, the system could threaten not only users’ free expression online, but also their privacy. Earlier this year, celebrity videoblogger Hank Green wrote a scathing critique of the way Facebook handles video content. Among other criticisms, Green said that Facebook hadn’t done enough to combat freebooting, the practice of downloading someone else’s video and reuploading it to your own profile. You’ve probably noticed freebooted videos on Facebook: they’re often the same funny videos you saw last month on YouTube, sometimes with crappy advertising or other text added to them. Freebooting is more popular on Facebook than you might think: Green cited a study showing that in the first quarter of 2015, 725 of the 1,000 most popular videos were freebooted. According to Green, Facebook implicitly rewards freebooting by prioritizing native video uploads over YouTube embeds: “[W]hen embedding a YouTube video on your company’s Facebook page is a sure way to see it die a sudden death, we shouldn’t be surprised when they rip it off YouTube and upload it natively. Facebook’s algorithms encourage this theft.” Green’s criticism struck a chord with a lot of content creators. People and companies that produced video wanted to know that Facebook had a plan to fight freebooting. The uproar came at an inconvenient time for Facebook, just as it was looking to ramp up its presence in the online video world and build better relationships with those same creators. In August, Facebook announced that it would be rolling out new features to combat unauthorized video sharing: [W]e have been building new video matching technology that will be available to a subset of creators. This technology is tailored to our platform, and will allow these creators to identify matches of their videos on Facebook across Pages, profiles, groups, and geographies. Our matching tool will evaluate millions of video uploads quickly and accurately, and when matches are surfaced, publishers will be able to report them to us for removal. Actually, Facebook has been building its content matching technology piecemeal for some time. For years, Facebook has partnered with Audible Magic, whose audio fingerprinting service is used by several social media sites to pinpoint copied music. Facebook has launched the new video matching system with a small group of content creators, with plans to roll it out to a larger base of users. Facebook’s announcement invites comparisons to YouTube’s sometimes-problematic Content ID service. Content ID lets rights holders submit large databases of video and audio fingerprints. The bot scans every new upload for potential matches to those fingerprints. The rights holder can choose whether to block, monetize, or monitor matching videos. Since the system can automatically remove or monetize a video with no human interaction, it can often remove videos that are clearly lawful fair uses and even videos that haven’t copied from another work at all. Fortunately, it appears that Facebook doesn’t currently remove anything automatically: it detects potential matches and flags them for the rights holder’s review. If the rights holder reports a video as potential copyright infringement, it’s still not deleted automatically. For now, all requests are reviewed by human staff at Facebook. We're glad that Facebook has introduced video matching in a way that won’t create unnecessary and annoying autotakedowns. But what happens when you share a video only with your friends (or with a private group) and that video registers as a match? From what we’ve pieced together, when you upload a video intended only for friends and Facebook thinks it might be a match, you won’t be allowed to share it with your friends unless you are willing to show it to the rights holder as well. (We haven’t actually seen this happen. If you see a notification like this when uploading a video to Facebook, please let us know about it.) The policy may be better than some of the alternatives. For example, it would be a disaster if Facebook sent the matching video to the rights holder without notifying the uploader, or if Facebook simply deleted private videos with no human review (as YouTube has been known to do). Still, Facebook has effectively created a new restriction for private communications: if you’re not willing to share your private video with someone whose copyright a computer thinks you might be infringing, you can’t share it with your friends. That’s unsettling for two reasons. First, it may put your privacy at risk. Rights holders can’t see your name, but there’s no way to scrub personally identifying information from the video itself. If you upload a very personal video that happens to have a Drake song playing in the background, it doesn’t make much sense to require you to share the video with Drake’s record label. Second, it could undermine fair use rights. The section of U.S. law that defines fair use specifically dictates that using parts of a work for the purpose of criticizing that work doesn’t constitute copyright infringement. It doesn’t require that that criticism be shared directly with the rights holder. It’s easy to think of scenarios in which an uploader wouldn’t want to share their criticism with the rights holder, or even ones in which doing so could be dangerous. For example, the target of criticism could be the uploader’s employer, or someone known for harassing their critics. Facebook is a great platform for video creators to privately share and discuss their work: what about mashup artists or activists using Facebook to share draft edits with each other? Does it make sense to require users to run their work by copyright holders as a condition of exercising their fair use rights? Here’s a better question: is it really necessary to run privately uploaded videos through the copyright bot at all? The simplest solution might be for Facebook not to scan private videos for matches. Scanning private communications for copyright infringement is foolish at best, and downright scary at worst (imagine the backlash if Google started using Content ID in Gmail). Uploading a video to share it with your friends is very different from sharing it publicly. When you share a video publicly, it can go viral and reach thousands of people. When you share it with your friends, it can only reach those friends. (Because of the way Facebook works, a video that you only share with your friends can’t even reach your friends’ friends: your friends can share your post, but only with people who are also on your friends list.) One of the four factors used to determine whether a certain use of a copyrighted work is protected under fair use is the impact that the use might have on the market for the original work. The impact that private sharing on Facebook has on the demand for a work is minimal. Unlike on YouTube, there’s no monetary reward for video views on Facebook. That’s not to say that no one uses Facebook commercially: many brands and content creators do, but their methods for monetizing their videos are more complicated than views alone. If a person wanted to use Rihanna’s videos to mislead viewers and compete for her album sales, he wouldn’t be very successful just sharing them with his friends. To return to the original criticisms of Facebook that led to the matching system, private sharing isn’t the issue at all. The freebooters that Hank Green is annoyed with are, by definition, uploading their videos publicly. Imagine trying to send an email and having your email service tell you it can’t send it because of an alleged copyright violation. In essence, that’s what Facebook’s video matching system does. If you can’t share a video privately without allowing a third party to view it, then you can’t share a video privately. If it sounds like we’re holding Facebook to a high standard; well, Facebook holds itself to a high standard. Facebook’s new Free Basics program brings a Facebook-centered version of the Internet to many mobile users in developing countries. Here in the U.S. and around the world, Facebook partners with mobile providers to offer special plans that include unlimited Facebook access. For millions of people, Facebook is the Internet. A Facebook policy that impacts people’s ability to communicate privately deserves extra scrutiny. A lot of the time, Facebook withstands scrutiny. For example, Facebook shows more respect for its users than some of its peers do in the face of governments’ demands for user data. In this case, though, Facebook is making a misstep. Going to reasonable lengths to earn content creators’ trust is a good thing, but when it gets in the way of private communications, it’s time to reevaluate. UPDATE 11-19-15: The trial of the seven Moroccan human rights workers has been delayed until January 2016, in part because of the increased international attention these cases have garnered. On November 19, the Moroccan government will put seven activists on trial as part of its ongoing crackdown on journalists and human rights defenders. EFF has joined a coalition of human rights organizations, including Free Press Unlimited, Article 19, and Mamfakinch to express our concern about the harassment and prosecution of these activists and to demand that all charges be dropped immediately. Maâti Monjib, Hicham Mansouri, Samad Iach, Mohamed Elsabr, and Hisham Almiraat (aka Hisham Khribchi) are all facing criminal charges of “threatening the internal security of the state.” If found guilty, they could face up to five years in prison. Rachid Tarek and Maria Moukrim are facing charges of “receiving foreign funding without notifying the General Secretariat of the government.” If found guilty, they could face fines. Dissidents in Morocco face continuous interrogations, threats, arrests, and surveillance. These prosecutions, meant to silence dissent against the Moroccan government, violate the defendants’ right to freedom of expression, guaranteed under Article 19 of the Universal Declaration of Human Rights. EFF and other organizations will be following the trial closely. Internet users generally think of YouTube as a platform where, if you play by the copyright rules, the content you post is safe from takedown and, if it's taken down improperly, you have some recourse. But that's not the case, thanks to an additional barrier to lawful sharing: meet YouTube's “contractual obligations.” YouTube has made special deals with certain rightsholders that allows them to dictate where and how their content can be used on the site. If your video uses content controlled by these rightsholders, and they object to that use, YouTube will take your video offline and won't restore it unless you can get the rightsholder's permission. Because the takedown isn't subject to the DMCA, the rightsholder has no legal obligations to consider whether your use is a lawful fair use. Given the importance of YouTube as a platform for expression, these deals can have dangerous consequences for online speech.  A case in point: the lengthy ordeal of YouTube channel LiberalViewer. The channel regularly engages in political commentary, and usually criticizes Fox News' presentation of news events. In one such video, uploaded in January 2008, LiberalViewer criticized the way Fox News abruptly cut away from a speech being delivered by then-candidate Obama. Back then, Obama's campaign frequently used Stevie Wonder's “Signed, Sealed, Delivered I'm Yours” after campaign speeches. So naturally, in explaining Fox News' cut away of Obama, LiberalViewer's video included a snippet of the song. Nothing happened for a few years, until November 2011, Universal Music Group issued a Content ID claim against the video. LiberalViewer immediately disputed the claim and contacted Universal Music Group's agent. After several emails exchanges, rather than lifting the claim, Universal responded with a DMCA takedown notice. Believing his video to be a lawful fair use, LiberalViewer counter-noticed. YouTube briefly reinstated the video, but then took it down again, claiming that the counter-notice was invalid because the user did not have “sufficient rights.” That's how things ended in 2012. In 2015, partly encouraged by our win in Lenz v. Universal, LiberalViewer once-again submitted a counter-notice. This time, YouTube rejected the counter-notice for a new reason:
  Universal Music has doubled down on its DMCA takedown notice and LiberalViewer has nowhere to turn at this point. The channel has little leverage with Universal, a rightholder that has not shown itself to be particularly interested in acknowledging online fair use. YouTube has washed its hands off of the whole affair. You can still view the video on LiberalViewer's site here (the video plays the song between the 1:42 - 2:16 marks for a total of 34 seconds). But it has been taken off a major platform, with no means of recourse. Given how much it owes its success to user contributions, it's a shame that YouTube has given some rightholders a private veto right on fair use. But it is Universal that has chosen to exercise that right. Thanks to that choice, Universal has earned its third entry into the EFF Takedown Hall of Shame. Last month, EFF and I scored a major victory for video game archiving, preservation and play – we got an exemption to the Digital Millennium Copyright Act for some archival activities related to video games. Before I throw a bunch of shade, I want to emphasize that the exemption is a victory for the video game archiving community. Although there were flaws in what the Library of Congress granted, more legal leeway in this space is a net positive. First, what the Librarian of Congress granted: an exemption for the circumvention of authentication servers in order to render games playable, so long as the game content is stored on the player’s computer or console. There’s now more legal protection for modifying a single player game where the authentication server has been deactivated for continued play or for preservation. So if and when Blizzard deactivates those Diablo III servers, players can modify their own games to continue playing. The exemption only covers “local gameplay,” which the Librarian defines as “gameplay conducted on a personal computer or video game console, or locally connected personally computers or consoles, and not through an online service or facility.” So to benefit from the exemption, a game may be modified to allow for local multiplayer play, but not online multiplayer. It’s unclear whether setting up a matchmaking server for local LAN play would be allowed under the current exemption – the definition suggests yes, as does the Copyright Office’s explanation, but it’s not specifically spelled out. Libraries, archives and museums (which I will collectively refer to as institutions), get more latitude. Under the exemption, they can eliminate access controls on video game consoles (often called “jailbreaking”) in order to copy and modify games to get them running again after a server shutdown. Still, activities that were under a legal cloud before are now protected. Surely we’re all going to rest on our laurels until 2 years from now? Nope. This round of exemptions revealed how fundamentally broken and unsustainable the triennial rulemaking has become. EFF, primarily Senior Staff Attorney Mitch Stoltz, and I spent countless hours preparing arguments for the gaming proposal, participating in hearings, and working with experts. Our proposal is one of 27 that the Librarian ruled upon, one of 7 that EFF participated in, and one of many advocated for by civil society groups. The rulemaking happens every three years, and the Librarian does not recognize any presumption that a previously-granted exemption should be renewed. Every three years, groups like EFF have to reapply for the same exemptions, marshaling up evidence and experts in order to produce a record of the potential harms. In some of the classes granted this year, the Librarian put a year-long delay on the exemption – meaning that it now only lasts two years. As if that wasn’t enough, the scope of the proceedings keeps expanding. Because so many things contain software, the DMCA now threatens legitimate activity on everything from tractors to medical devices. The video game archiving and preservation exemption at least involved alleged harms that were ostensibly related to copyright – the same can’t be said of car hacking or pacemaker security. As the scope expands, it becomes incredibly difficult to offer information that is both specific enough for the Register of Copyrights and broad enough to cover the variety of different lawful activities inhibited by the anti-circumvention law. This is not made any easier by the way that the Copyright Office arbitrarily imposes burdens upon the proponents. For example,an exemption to the anti-circumvention rule, §1201(a)(1), does not exempt you from other parts of the statute, including §1201(a)(2), which prohibits trafficking in circumvention tools. As such, the groups seeking exemptions don’t devote much time discussing the anti-trafficking provision as part of the triennial process – because even a granted exemption won’t allow an owner to traffic in a circumvention tool. However, in deciding to allow the restoration of multiplayer play under the video game exemption, the Register said that we failed to “provide persuasive support for an exemption for online multiplayer play, in large part because it is not clear on the current record how the provision of circumvention tools to multiple users to facilitate an alternative matchmaking service could be accomplished without running afoul of the anti-trafficking provision in section 1201(a)(2).” The issue of the matchmaking servers running afoul of 1201(a)(2) did not come up in any of the comments for the rulemaking. Not in the first filing, not when Copyright Office staff asked follow-up questions, not from the opposing briefs from the ESA, and not at the hearings. It was a surprise for the exemption to be denied on the basis that the proposed activity might possibly violate a legal provision that is not at issue in the rulemaking, particularly since the issue was never presented or briefed. This puts exemption proponents in a strange position. It suggests that people seeking exemptions may have to show that the behavior contemplated by the exemption is not just non-infringing, but that it is otherwise legal. Will proponents of a medical device exemption have to brief which uses do and do not implicate FDA regulations? And vehicle tinkerers brief every law that governs the modification or use of a vehicle? What about giving the courts a chance to determine the scope of 1201(a)(2) by removing the threat of simultaneous 1201(a)(1) liability? As one of the proponents, I can definitively say that we were able to provide far more evidence of communities that wished to restore multiplayer access to games where servers had been deactivated than of single-player shutdowns. The decision to exclude multiplayer play from the exemption cannot have been motivated by a lack of evidence that multiplayer shutdowns harm communities and consumers. The Register could find that the countervailing harms of illegal copying based upon circumvention are too great. (I would, of course, disagree.) But not granting the exemption because game enthusiasts might possibly violate a different part of the statute, without informing proponents that they bore the burden of proving non-violation of that provision, is unacceptable. The granted exemption gives an archival institution more latitude surrounding the jailbreaking of consoles, but requires in exchange that the institution does not distribute or make available the video game outside its physical premises. It’s unclear how distributing the video game outside the premises of the institution would make it more likely that the circumvention of the access controls would result in infringement. Remember, by the time the video game access controls have been circumvented, a) the server is down, by definition and b) we assume the institution is not trafficking in the circumvention tools because that would be illegal. (Although we didn’t have to prove that no institution would violate the trafficking provision, so don’t ask me how that works.) Second, and perhaps most importantly, how does this work in practice? Does the distribution of the game outside the physical premises of the institution make the Section 1201 violation retroactively illegal? Does the legality have to do with the intent that the institution had at the time of the potential circumvention? How do these conditions actually relate to the legal regime? ¯\_(ツ)_/¯ Furthermore, physical premises are not the only or even the primary medium through which institutions interact with the public. The future is not in-person attendance at museums. But now the legality of actions that museums took in the past is somehow tied to their physical premises. The Librarian of Congress granted a number of complex and specific exemptions, applying to a small field. They clear up legal uncertainty in some ways but create it in others. And that’s why any celebration of these DMCA exemption victories must be tempered with the knowledge that the process is broken. The Register made a number of compromises on many of the exemptions, designed to find a middle ground between proponents and opponents. That eliminates much of the legal clarity that the exemptions are meant to provide. As Sarah Jeong said on Twitter, it’s like the Copyright Office forgot that the moral of the story of King Solomon and the baby is “Don’t split the baby.” Because if you split the baby, the baby dies. Kendra Albert is a student at Harvard Law School. She was an EFF Legal Intern in summer 2014. No posts found
